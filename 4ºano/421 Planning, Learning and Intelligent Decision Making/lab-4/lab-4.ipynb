{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning and Decision Making"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Laboratory 4: Reinforcement learning\n",
    "\n",
    "In the end of the lab, you should export the notebook to a Python script (``File >> Download as >> Python (.py)``). Make sure that the resulting script includes all code written in the tasks marked as \"**Activity n. N**\", together with any replies to specific questions posed. Your file should be named `padi-labKK-groupXXX.py`, where `KK` corresponds to the lab number and the `XXX` corresponds to your group number. Similarly, your homework should consist of a single pdf file named `padi-hwKK-groupXXX.pdf`. You should create a zip file with the lab and homework files and submit it in Fenix **at most 30 minutes after your lab is over**.\n",
    "\n",
    "Make sure to strictly respect the specifications in each activity, in terms of the intended inputs, outputs and naming conventions.\n",
    "\n",
    "In particular, after completing the activities you should be able to replicate the examples provided (although this, in itself, is no guarantee that the activities are correctly completed)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. The MDP Model \n",
    "\n",
    "In this lab you will implement several reinforcement learning algorithms, and use the taxi domain from Lab 2 to test and compare these algorithms. Don't forget, however, that your functions should work for **any MDP** and not just the one provided. \n",
    "\n",
    "The taxi domain to be used is represented in the diagram below.\n",
    "\n",
    "<img src=\"taxi.png\" width=\"250px\">\n",
    "\n",
    "In the above domain, \n",
    "\n",
    "* The taxi can be in any of the 25 cells in the diagram. The passenger can be at any of the 4 marked locations ($Y$, $B$, $G$, $R$) or in the taxi. Additionally, the passenger wishes to go to one of the 4 possible destinations. The total number of states, in this case, is $25\\times 5\\times 4$.\n",
    "* At each step, the agent (taxi driver) may move in any of the four directions -- south, north, east and west. It can also pickup the passenger or drop off the passenger. \n",
    "* The goal of the taxi driver is to pickup the passenger and drop it at the passenger's desired destination.\n",
    "\n",
    "**Throughout the lab, unless if stated otherwise, use $\\gamma=0.99$.**\n",
    "\n",
    "$$\\diamond$$\n",
    "\n",
    "We start by loading the MDP for the taxi domain from the file `taxi.npz`. We will use this domain as an example to illustrate the different functions/algorithms you are expected to deploy. The file contains both the MDP, described as a tuple like those from Lab 2, and the corresponding optimal $Q$-function.\n",
    "\n",
    "To do so, you can run the code\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "mdp_info = np.load('taxi.npz', allow_pickle=True)\n",
    "\n",
    "# The MDP is a tuple (X, A, P, c, gamma)\n",
    "M = tuple(mdp_info['M'])\n",
    "\n",
    "# We also load the optimal Q-function for the MDP\n",
    "Qopt = mdp_info['Q']\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "In the first activity, you will implement a \"simulator of the world\". The simulator consists of a function that enables you to sample a transition from a given MDP. You will then use this function, in subsequent activities, to generate the data that your agent will use to learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Activity 1.        \n",
    "\n",
    "Write a function named `sample_transition` that receives, as input, a tuple representing an arbitrary MDP as well as two integers, `x` and `a`, corresponding to a state and an action. The function should return a tuple `(x, a, c, x')`, where `c` is the cost associated with performing action `a` in state `x` and `x'` is a state generated from `x` upon selecting action `a`, according to the transition probabilities for the MDP.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "mdp_info = np.load('taxi.npz', allow_pickle=True)\n",
    "\n",
    "# The MDP is a tuple (X, A, P, c, gamma)\n",
    "M = tuple(mdp_info['M'])\n",
    "\n",
    "# We also load the optimal Q-function for the MDP\n",
    "Qopt = mdp_info['Q']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T09:18:48.272364Z",
     "start_time": "2019-12-09T09:18:48.264410Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observed transition:\n",
      "((9, B, B), West, 0.7, (8, B, B))\n",
      "\n",
      "Observed transition:\n",
      "((10, G, B), Pickup, 1.0, (10, G, B))\n",
      "\n",
      "Observed transition:\n",
      "((4, Y, G), East, 0.7, (5, Y, G))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sample_transition(mdp:tuple, x:int, a:int):\n",
    "    X, A, P, c, gamma = mdp \n",
    "    cost = c[x][a]\n",
    "    x_new = rnd.choice(np.where(P[a][x] == np.max(P[a][x]))[0])\n",
    "    return x, a, cost, x_new\n",
    "\n",
    "\n",
    "import numpy.random as rnd\n",
    "\n",
    "rnd.seed(42)\n",
    "\n",
    "# Select random state and action\n",
    "x = 175 # State (9, B, B)\n",
    "a = rnd.randint(len(M[1]))\n",
    "\n",
    "x, a, cnew, xnew = sample_transition(M, x, a)\n",
    "\n",
    "print('Observed transition:\\n(', end='')\n",
    "print(M[0][x], end=', ')\n",
    "print(M[1][a], end=', ')\n",
    "print(cnew, end=', ')\n",
    "print(M[0][xnew], end=')\\n')\n",
    "\n",
    "# Select random state and action\n",
    "x = 187 # State (10, G, B)\n",
    "a = rnd.randint(len(M[1]))\n",
    "\n",
    "x, a, cnew, xnew = sample_transition(M, x, a)\n",
    "\n",
    "print('\\nObserved transition:\\n(', end='')\n",
    "print(M[0][x], end=', ')\n",
    "print(M[1][a], end=', ')\n",
    "print(cnew, end=', ')\n",
    "print(M[0][xnew], end=')\\n')\n",
    "\n",
    "# Select random state and action\n",
    "x = 69 # State (4, Y, G)\n",
    "a = rnd.randint(len(M[1]))\n",
    "\n",
    "x, a, cnew, xnew = sample_transition(M, x, a)\n",
    "\n",
    "print('\\nObserved transition:\\n(', end='')\n",
    "print(M[0][x], end=', ')\n",
    "print(M[1][a], end=', ')\n",
    "print(cnew, end=', ')\n",
    "print(M[0][xnew], end=')\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All reinforcement learning algorithms that you will implement can only access the MDP through the function `sample_transition` which, in a sense, simulates an \"interaction\" of the agent with the environment.\n",
    "\n",
    "For example, using the taxi MDP, you could run:\n",
    "\n",
    "```python\n",
    "import numpy.random as rnd\n",
    "\n",
    "rnd.seed(42)\n",
    "\n",
    "# Select random state and action\n",
    "x = 175 # State (9, B, B)\n",
    "a = rnd.randint(len(M[1]))\n",
    "\n",
    "x, a, cnew, xnew = sample_transition(M, x, a)\n",
    "\n",
    "print('Observed transition:\\n(', end='')\n",
    "print(M[0][x], end=', ')\n",
    "print(M[1][a], end=', ')\n",
    "print(cnew, end=', ')\n",
    "print(M[0][xnew], end=')\\n')\n",
    "\n",
    "# Select random state and action\n",
    "x = 187 # State (10, G, B)\n",
    "a = rnd.randint(len(M[1]))\n",
    "\n",
    "x, a, cnew, xnew = sample_transition(M, x, a)\n",
    "\n",
    "print('\\nObserved transition:\\n(', end='')\n",
    "print(M[0][x], end=', ')\n",
    "print(M[1][a], end=', ')\n",
    "print(cnew, end=', ')\n",
    "print(M[0][xnew], end=')\\n')\n",
    "\n",
    "# Select random state and action\n",
    "x = 69 # State (4, Y, G)\n",
    "a = rnd.randint(len(M[1]))\n",
    "\n",
    "x, a, cnew, xnew = sample_transition(M, x, a)\n",
    "\n",
    "print('\\nObserved transition:\\n(', end='')\n",
    "print(M[0][x], end=', ')\n",
    "print(M[1][a], end=', ')\n",
    "print(cnew, end=', ')\n",
    "print(M[0][xnew], end=')\\n')\n",
    "```\n",
    "\n",
    "and get, as output:\n",
    "\n",
    "```\n",
    "Observed transition:\n",
    "((9, B, B), West, 0.7, (8, B, B))\n",
    "\n",
    "Observed transition:\n",
    "((10, G, B), East, 0.7, (10, G, B))\n",
    "\n",
    "Observed transition:\n",
    "((4, Y, G), Pickup, 1.0, (4, Y, G))\n",
    "```\n",
    "\n",
    "**Note:** For debug purposes, we also provide a second file, `taxi-small.npz`, that contains a 9-state MDP that you can use to verify if your results make sense."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "raw_mimetype": "text/latex"
   },
   "source": [
    "---\n",
    "\n",
    "#### Activity 2.        \n",
    "\n",
    "Write down a function named `egreedy` that implements an $\\epsilon$-greedy policy. Your function should receive, as input, a `numpy` array `Q` with shape `(N,)`, for some integer `N`, and, as an optional argument, a floating point number `eps` with a default value `eps=0.1`. Your function should return... \n",
    "\n",
    "* ... with a probability $\\epsilon$, a random index between $0$ and $N-1$.\n",
    "* ... with a probability $1-\\epsilon$, the index between $0$ and $N-1$ corresponding to the minimum value of `Q`. If more than one such index exists, the function should select among such indices **uniformly at random**.\n",
    "\n",
    "**Note:** In the upcoming activities, the array `Q` received by the function `egreedy` will correspond to a row of a $Q$-function, and `N` will correspond to the number of actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T09:18:48.301639Z",
     "start_time": "2019-12-09T09:18:48.296224Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State: (9, B, B) - action (eps=0.0): South\n",
      "State: (9, B, B) - action (eps=0.5): South\n",
      "State: (9, B, B) - action (eps=1.0): Pickup\n",
      "\n",
      "State: (10, G, B) - action (eps=0.0): North\n",
      "State: (10, G, B) - action (eps=0.5): East\n",
      "State: (10, G, B) - action (eps=1.0): Pickup\n",
      "\n",
      "State: (4, Y, G) - action (eps=0.0): West\n",
      "State: (4, Y, G) - action (eps=0.5): South\n",
      "State: (4, Y, G) - action (eps=1.0): West\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def egreedy(Q, eps: float=0.1):\n",
    "    if rnd.random() < eps:\n",
    "        return rnd.choice(range(len(Q)))\n",
    "    else:\n",
    "        return rnd.choice(np.where(Q == np.min(Q))[0])\n",
    "\n",
    "rnd.seed(42)\n",
    "\n",
    "x = 175 # State (9, B, B)\n",
    "a = egreedy(Qopt[x, :], eps=0)\n",
    "print('State:', M[0][x], '- action (eps=0.0):', M[1][a])\n",
    "a = egreedy(Qopt[x, :], eps=0.5)\n",
    "print('State:', M[0][x], '- action (eps=0.5):', M[1][a])\n",
    "a = egreedy(Qopt[x, :], eps=1.0)\n",
    "print('State:', M[0][x], '- action (eps=1.0):', M[1][a])\n",
    "\n",
    "x = 187 # State (10, G, B)\n",
    "a = egreedy(Qopt[x, :], eps=0)\n",
    "print('\\nState:', M[0][x], '- action (eps=0.0):', M[1][a])\n",
    "a = egreedy(Qopt[x, :], eps=0.5)\n",
    "print('State:', M[0][x], '- action (eps=0.5):', M[1][a])\n",
    "a = egreedy(Qopt[x, :], eps=1.0)\n",
    "print('State:', M[0][x], '- action (eps=1.0):', M[1][a])\n",
    "\n",
    "x = 69 # State (4, Y, G)\n",
    "a = egreedy(Qopt[x, :], eps=0)\n",
    "print('\\nState:', M[0][x], '- action (eps=0.0):', M[1][a])\n",
    "a = egreedy(Qopt[x, :], eps=0.5)\n",
    "print('State:', M[0][x], '- action (eps=0.5):', M[1][a])\n",
    "a = egreedy(Qopt[x, :], eps=1.0)\n",
    "print('State:', M[0][x], '- action (eps=1.0):', M[1][a])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, using the function `Qopt` loaded from the taxi file, you can run:\n",
    "\n",
    "```python\n",
    "rnd.seed(42)\n",
    "\n",
    "x = 175 # State (9, B, B)\n",
    "a = egreedy(Qopt[x, :], eps=0)\n",
    "print('State:', M[0][x], '- action (eps=0.0):', M[1][a])\n",
    "a = egreedy(Qopt[x, :], eps=0.5)\n",
    "print('State:', M[0][x], '- action (eps=0.5):', M[1][a])\n",
    "a = egreedy(Qopt[x, :], eps=1.0)\n",
    "print('State:', M[0][x], '- action (eps=1.0):', M[1][a])\n",
    "\n",
    "x = 187 # State (10, G, B)\n",
    "a = egreedy(Qopt[x, :], eps=0)\n",
    "print('\\nState:', M[0][x], '- action (eps=0.0):', M[1][a])\n",
    "a = egreedy(Qopt[x, :], eps=0.5)\n",
    "print('State:', M[0][x], '- action (eps=0.5):', M[1][a])\n",
    "a = egreedy(Qopt[x, :], eps=1.0)\n",
    "print('State:', M[0][x], '- action (eps=1.0):', M[1][a])\n",
    "\n",
    "x = 69 # State (4, Y, G)\n",
    "a = egreedy(Qopt[x, :], eps=0)\n",
    "print('\\nState:', M[0][x], '- action (eps=0.0):', M[1][a])\n",
    "a = egreedy(Qopt[x, :], eps=0.5)\n",
    "print('State:', M[0][x], '- action (eps=0.5):', M[1][a])\n",
    "a = egreedy(Qopt[x, :], eps=1.0)\n",
    "print('State:', M[0][x], '- action (eps=1.0):', M[1][a])\n",
    "```\n",
    "\n",
    "and you will get the output:\n",
    "\n",
    "```\n",
    "State: (9, B, B) - action (eps=0.0): South\n",
    "State: (9, B, B) - action (eps=0.5): South\n",
    "State: (9, B, B) - action (eps=1.0): East\n",
    "\n",
    "State: (10, G, B) - action (eps=0.0): North\n",
    "State: (10, G, B) - action (eps=0.5): East\n",
    "State: (10, G, B) - action (eps=1.0): North\n",
    "\n",
    "State: (4, Y, G) - action (eps=0.0): West\n",
    "State: (4, Y, G) - action (eps=0.5): West\n",
    "State: (4, Y, G) - action (eps=1.0): West\n",
    "```\n",
    "\n",
    "**Note that, depending on the order and number of calls to functions in the random library you may get slightly different results.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Activity 3. \n",
    "\n",
    "Write a function `mb_learning` that implements the model-based reinforcement learning algorithm discussed in class. Your function should receive as input arguments \n",
    "\n",
    "* A tuple, `mdp`, containing the description of an **arbitrary** MDP. The structure of the tuple is similar to that provided in the example above. \n",
    "* An integer, `n`, corresponding the number of steps that your algorithm should run.\n",
    "*  A numpy array `qinit` with as many rows as the number of states in `mdp` and as many columns as the number of actions in `mdp`. The matrix `qinit` should be used to initialize the $Q$-function being learned by your function.\n",
    "* A tuple, `Pinit`, with as many elements as the number of actions in `mdp`. Each element of `Pinit` corresponds to square numpy arrays with as many rows/columns as the number of states in `mdp` and can be **any** transition probability matrix. The matrices in `Pinit` should be used to initialize the transition probability matrices of the model being learned by your function.\n",
    "* A numpy array `cinit` with as many rows as the number of states in `mdp` and as many columns as the number of actions in `mdp`. The matrix `cinit` should be used to initialize the cost function of the model being learned by your function.\n",
    "\n",
    "Your function should simulate an interaction of `n` steps between the agent and the environment, starting from an arbitrary state chosen uniformly at random, and during which it should perform `n` iterations of the model-based RL algorithm seen in class. In particular, it should learn the transition probabilities and cost function from the interaction between the agent and the environment, and use these to compute the optimal $Q$-function. The transition probabilities, cost and $Q$-functions to be learned should be initialized using `Pinit`, `cinit` and `qinit`, respectively. \n",
    "\n",
    "Note that, at each step of the interaction,\n",
    "\n",
    "* The agent should observe the current state, and select an action using an $\\epsilon$-greedy policy with respect to its current estimate of the optimal $Q$-values. You should use the function `egreedy` from Activity 2, with $\\epsilon=0.15$. \n",
    "* Given the state and action, you must then compute the cost and generate the next state, using `mdp` and the function `sample_transition` from Activity 1.\n",
    "* With this transition information (state, action, cost, next-state), you can now perform an update to the transition probabilities, cost function, and $Q$-function.\n",
    "* When updating the components $(x,a)$ of the model, use the step-size\n",
    "\n",
    "$$\\alpha_t=\\frac{1}{N_t(x,a)+1},$$\n",
    "\n",
    "where $N_t(x,a)$ is the number of visits to the pair $(x,a)$ up to time step $t$.\n",
    "\n",
    "Your function should return a tuple containing:\n",
    "\n",
    "*  A numpy array with as many rows as the number of states in `mdp` and as many columns as the number of actions in `mdp`, corresponding to the learned $Q$-function.\n",
    "* A tuple with as many elements as the number of actions in `mdp`. The element $a$ of the tuple corresponds to a square numpy array with as many rows/columns as the number of states in `mdp`, corresponding to the learned transition probabilities for action $a$.\n",
    "* A numpy array with as many rows as the number of states in `mdp` and as many columns as the number of actions in `mdp`, corresponding to the learned cost function.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T09:18:48.330597Z",
     "start_time": "2019-12-09T09:18:48.322311Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in Q after 1000 steps: 388.72130940745296\n",
      "Error in Q after 2000 steps: 378.4927134561178\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import numpy.random as rnd\n",
    "def mb_learning(mdp:tuple, n:int, qinit:np.array, Pinit:tuple, cinit:np.array):\n",
    "    X, A, P_mdp, c_mdp, gamma = mdp\n",
    "    x = rnd.randint(len(X))\n",
    "    N = len(A)\n",
    "    Q = qinit\n",
    "    P = Pinit\n",
    "    c = cinit\n",
    "    N_t = np.zeros((len(X), N))\n",
    "    for _ in range(n):\n",
    "        a = egreedy(Q[x], 0.15)\n",
    "        _, _, cost, x_new= sample_transition(mdp, x, a)\n",
    "\n",
    "        alpha = 1 / (N_t[x][a] + 1)\n",
    "        P += alpha * (np.eye(len(X))[x_new] - P)\n",
    "        c += alpha * (cost - c) \n",
    "\n",
    "        N_t[x][a] += 1\n",
    "\n",
    "        Q_next_min = np.min(Q, axis=1)\n",
    "        Q_next = np.dot(P[a][x], Q_next_min)\n",
    "        \n",
    "        # Update Q-values using the provided equation\n",
    "        Q[x][a] = c[x][a] + gamma * np.sum(Q_next)\n",
    "        x = x_new\n",
    "    return Q, tuple(P), c\n",
    "\n",
    "\n",
    "rnd.seed(42)\n",
    "\n",
    "# Initialize transition probabilities\n",
    "pinit = ()\n",
    "\n",
    "for a in range(len(M[1])):\n",
    "    pinit += (np.eye(len(M[0])),)\n",
    "\n",
    "# Initialize cost function\n",
    "cinit = np.zeros((len(M[0]), len(M[1])))\n",
    "\n",
    "# Initialize Q-function\n",
    "qinit = np.zeros((len(M[0]), len(M[1])))\n",
    "\n",
    "# Run 1000 steps of model-based learning\n",
    "qnew, pnew, cnew = mb_learning(M, 1000, qinit, pinit, cinit)\n",
    "\n",
    "# Compare the learned Q with the optimal Q\n",
    "print('Error in Q after 1000 steps:', np.linalg.norm(qnew - Qopt))\n",
    "\n",
    "# Run 1000 additional steps of model-based learning\n",
    "qnew, pnew, cnew = mb_learning(M, 1000, qnew, pnew, cnew)\n",
    "\n",
    "# Compare once again the learned Q with the optimal Q\n",
    "print('Error in Q after 2000 steps:', np.linalg.norm(qnew - Qopt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T09:18:48.567188Z",
     "start_time": "2019-12-09T09:18:48.333226Z"
    },
    "scrolled": false
   },
   "source": [
    "As an example using the taxi MDP, we could run:\n",
    "\n",
    "```python\n",
    "rnd.seed(42)\n",
    "\n",
    "# Initialize transition probabilities\n",
    "pinit = ()\n",
    "\n",
    "for a in range(len(M[1])):\n",
    "    pinit += (np.eye(len(M[0])),)\n",
    "\n",
    "# Initialize cost function\n",
    "cinit = np.zeros((len(M[0]), len(M[1])))\n",
    "\n",
    "# Initialize Q-function\n",
    "qinit = np.zeros((len(M[0]), len(M[1])))\n",
    "\n",
    "# Run 1000 steps of model-based learning\n",
    "qnew, pnew, cnew = mb_learning(M, 1000, qinit, pinit, cinit)\n",
    "\n",
    "# Compare the learned Q with the optimal Q\n",
    "print('Error in Q after 1000 steps:', np.linalg.norm(qnew - Qopt))\n",
    "\n",
    "# Run 1000 additional steps of model-based learning\n",
    "qnew, pnew, cnew = mb_learning(M, 1000, qnew, pnew, cnew)\n",
    "\n",
    "# Compare once again the learned Q with the optimal Q\n",
    "print('Error in Q after 2000 steps:', np.linalg.norm(qnew - Qopt))\n",
    "```\n",
    "\n",
    "to get\n",
    "\n",
    "```\n",
    "Error in Q after 1000 steps: 388.8249426163114\n",
    "Error in Q after 2000 steps: 388.8221418599741\n",
    "```\n",
    "\n",
    "Note that, even if the seed is fixed, the numerical values may differ somewhat from those above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Model-free learning\n",
    "\n",
    "You will now implement both $Q$-learning and SARSA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Activity 4. \n",
    "\n",
    "Write a function `qlearning` that implements the $Q$-learning algorithm discussed in class. Your function should receive as input arguments \n",
    "\n",
    "* A tuple, `mdp`, containing the description of an **arbitrary** MDP. The structure of the tuple is similar to that provided in the examples above. \n",
    "* An integer, `n`, corresponding he number of steps that your algorithm should run.\n",
    "*  A `numpy` array `qinit` with as many rows as the number of states in `mdp` and as many columns as the number of actions in `mdp`. The matrix `qinit` should be used to initialize the $Q$-function being learned by your function.\n",
    "\n",
    "Your function should simulate an interaction of `n` steps between the agent and the environment, starting from an arbitrary state chosen uniformly at random, and during which it should perform `n` iterations of the $Q$-learning algorithm seen in class. In particular, it should learn optimal $Q$-function. The $Q$-function to be learned should be initialized using `qinit`. \n",
    "\n",
    "Note that, at each step of the interaction,\n",
    "\n",
    "* The agent should observe the current state, and select an action using an $\\epsilon$-greedy policy with respect to its current estimate of the optimal $Q$-values. You should use the function `egreedy` from Activity 2, with $\\epsilon=0.15$. \n",
    "* Given the state and action, you must then compute the cost and generate the next state, using `mdp` and the function `sample_transition` from Activity 1.\n",
    "* With this transition information (state, action, cost, next-state), you can now perform an update to the $Q$-function.\n",
    "* When updating the components $(x,a)$ of the model, use the step-size $\\alpha=0.3$.\n",
    "\n",
    "Your function should return a `numpy` array with as many rows as the number of states in `mdp` and as many columns as the number of actions in `mdp`, corresponding to the learned $Q$-function.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T09:18:48.576851Z",
     "start_time": "2019-12-09T09:18:48.571201Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in Q after 1000 steps: 389.1083239578609\n",
      "Error in Q after 1000 steps: 388.53818164643576\n",
      "Error in Q after 2000 steps: 387.27960171929\n",
      "Error in Q after 3000 steps: 387.26698944320395\n",
      "Error in Q after 4000 steps: 382.22769426446564\n",
      "Error in Q after 5000 steps: 381.4659502446264\n",
      "Error in Q after 6000 steps: 376.8869406039795\n",
      "Error in Q after 7000 steps: 371.7586335968834\n",
      "Error in Q after 8000 steps: 363.9284526840498\n",
      "Error in Q after 9000 steps: 350.93058337861055\n",
      "Error in Q after 10000 steps: 346.89064043324396\n",
      "Error in Q after 11000 steps: 342.25757353412934\n",
      "Error in Q after 12000 steps: 338.95639253960036\n",
      "Error in Q after 13000 steps: 338.92942619645606\n",
      "Error in Q after 14000 steps: 334.19570285869946\n",
      "Error in Q after 15000 steps: 331.0042237682255\n",
      "Error in Q after 16000 steps: 327.64057973224345\n",
      "Error in Q after 17000 steps: 319.4432475399161\n",
      "Error in Q after 18000 steps: 312.1857191407452\n",
      "Error in Q after 19000 steps: 309.27612489426593\n",
      "Error in Q after 20000 steps: 309.27612489426593\n",
      "Error in Q after 21000 steps: 308.34823250463444\n",
      "Error in Q after 22000 steps: 307.49557143559474\n",
      "Error in Q after 23000 steps: 307.25510728301595\n",
      "Error in Q after 24000 steps: 305.9520328937533\n",
      "Error in Q after 25000 steps: 304.40469992653937\n",
      "Error in Q after 26000 steps: 304.37052023369966\n",
      "Error in Q after 27000 steps: 304.1981845612287\n",
      "Error in Q after 28000 steps: 299.85507927854\n",
      "Error in Q after 29000 steps: 299.39753733925403\n",
      "Error in Q after 30000 steps: 297.99090922367185\n",
      "Error in Q after 31000 steps: 297.99090922367185\n",
      "Error in Q after 32000 steps: 295.5874821448392\n",
      "Error in Q after 33000 steps: 293.00992301261726\n",
      "Error in Q after 34000 steps: 290.63962065355213\n",
      "Error in Q after 35000 steps: 287.15733852741374\n",
      "Error in Q after 36000 steps: 286.70586263883763\n",
      "Error in Q after 37000 steps: 286.183672939906\n",
      "Error in Q after 38000 steps: 282.9792092165674\n",
      "Error in Q after 39000 steps: 282.40291703214615\n",
      "Error in Q after 40000 steps: 279.94292247140896\n",
      "Error in Q after 41000 steps: 277.1763900764267\n",
      "Error in Q after 42000 steps: 274.2146111073621\n",
      "Error in Q after 43000 steps: 271.17128296358385\n",
      "Error in Q after 44000 steps: 270.90233782818154\n",
      "Error in Q after 45000 steps: 269.8524141180149\n",
      "Error in Q after 46000 steps: 269.7960742283876\n",
      "Error in Q after 47000 steps: 262.11166728343295\n",
      "Error in Q after 48000 steps: 260.458428819201\n",
      "Error in Q after 49000 steps: 259.12609375350235\n",
      "Error in Q after 50000 steps: 258.55585464884354\n",
      "Error in Q after 51000 steps: 257.23465068926754\n",
      "Error in Q after 52000 steps: 255.30466318392592\n",
      "Error in Q after 53000 steps: 252.82237416738135\n",
      "Error in Q after 54000 steps: 251.62119551933588\n",
      "Error in Q after 55000 steps: 249.82302766279528\n",
      "Error in Q after 56000 steps: 249.04021046290308\n",
      "Error in Q after 57000 steps: 249.04021046290308\n",
      "Error in Q after 58000 steps: 248.00533523494698\n",
      "Error in Q after 59000 steps: 245.61489605181077\n",
      "Error in Q after 60000 steps: 243.31668299472568\n",
      "Error in Q after 61000 steps: 241.9095844458707\n",
      "Error in Q after 62000 steps: 240.69556023337088\n",
      "Error in Q after 63000 steps: 240.2807366071741\n",
      "Error in Q after 64000 steps: 240.08201046028796\n",
      "Error in Q after 65000 steps: 235.38151596018554\n",
      "Error in Q after 66000 steps: 232.3475825690442\n",
      "Error in Q after 67000 steps: 232.11714531514767\n",
      "Error in Q after 68000 steps: 230.40587445712083\n",
      "Error in Q after 69000 steps: 229.67938948850585\n",
      "Error in Q after 70000 steps: 229.32422082205332\n",
      "Error in Q after 71000 steps: 229.27522724076206\n",
      "Error in Q after 72000 steps: 228.14253450488104\n",
      "Error in Q after 73000 steps: 225.8606681470177\n",
      "Error in Q after 74000 steps: 224.57999783898958\n",
      "Error in Q after 75000 steps: 221.6364121286181\n",
      "Error in Q after 76000 steps: 220.71447116767985\n",
      "Error in Q after 77000 steps: 219.78498083998983\n",
      "Error in Q after 78000 steps: 218.52322917347502\n",
      "Error in Q after 79000 steps: 217.5825197177165\n",
      "Error in Q after 80000 steps: 216.98067901112222\n",
      "Error in Q after 81000 steps: 216.8926398691879\n",
      "Error in Q after 82000 steps: 215.62264476623508\n",
      "Error in Q after 83000 steps: 215.56107659234786\n",
      "Error in Q after 84000 steps: 214.5575083492579\n",
      "Error in Q after 85000 steps: 214.17531397226588\n",
      "Error in Q after 86000 steps: 212.33940832103374\n",
      "Error in Q after 87000 steps: 211.01814831745943\n",
      "Error in Q after 88000 steps: 210.57838178077054\n",
      "Error in Q after 89000 steps: 210.1135117170333\n",
      "Error in Q after 90000 steps: 209.0816119321105\n",
      "Error in Q after 91000 steps: 207.23872829641553\n",
      "Error in Q after 92000 steps: 205.6594005190573\n",
      "Error in Q after 93000 steps: 204.57105887920747\n",
      "Error in Q after 94000 steps: 202.90368545472379\n",
      "Error in Q after 95000 steps: 199.21992265881914\n",
      "Error in Q after 96000 steps: 198.09462005865285\n",
      "Error in Q after 97000 steps: 196.65890387905736\n",
      "Error in Q after 98000 steps: 195.57797928058633\n",
      "Error in Q after 99000 steps: 195.24535927774886\n",
      "Error in Q after 100000 steps: 194.81398587586293\n",
      "Error in Q after 101000 steps: 194.64532082833975\n",
      "Error in Q after 102000 steps: 192.90994695515113\n",
      "Error in Q after 103000 steps: 192.89611795837274\n",
      "Error in Q after 104000 steps: 191.82715534084167\n",
      "Error in Q after 105000 steps: 190.78676768999543\n",
      "Error in Q after 106000 steps: 188.66934142155742\n",
      "Error in Q after 107000 steps: 186.51577096512918\n",
      "Error in Q after 108000 steps: 185.57045534274252\n",
      "Error in Q after 109000 steps: 184.3552981105523\n",
      "Error in Q after 110000 steps: 183.99795822424127\n",
      "Error in Q after 111000 steps: 182.2495134258257\n",
      "Error in Q after 112000 steps: 179.76696074573942\n",
      "Error in Q after 113000 steps: 179.44242676873847\n",
      "Error in Q after 114000 steps: 179.14819657596706\n",
      "Error in Q after 115000 steps: 178.60129472900738\n",
      "Error in Q after 116000 steps: 177.96081020677354\n",
      "Error in Q after 117000 steps: 177.7104083611642\n",
      "Error in Q after 118000 steps: 177.71694608629338\n",
      "Error in Q after 119000 steps: 177.75421915322872\n",
      "Error in Q after 120000 steps: 177.25915045474053\n",
      "Error in Q after 121000 steps: 177.27476157584252\n",
      "Error in Q after 122000 steps: 176.54965721911537\n",
      "Error in Q after 123000 steps: 174.67357808688067\n",
      "Error in Q after 124000 steps: 174.23709562849533\n",
      "Error in Q after 125000 steps: 174.26451107217247\n",
      "Error in Q after 126000 steps: 174.103004517958\n",
      "Error in Q after 127000 steps: 174.1668071164818\n",
      "Error in Q after 128000 steps: 173.59372963319862\n",
      "Error in Q after 129000 steps: 172.4547763751375\n",
      "Error in Q after 130000 steps: 172.11455700157427\n",
      "Error in Q after 131000 steps: 170.88309820427693\n",
      "Error in Q after 132000 steps: 170.20769794701116\n",
      "Error in Q after 133000 steps: 170.04955403286522\n",
      "Error in Q after 134000 steps: 170.05353557426227\n",
      "Error in Q after 135000 steps: 170.05530262685838\n",
      "Error in Q after 136000 steps: 167.0373553427037\n",
      "Error in Q after 137000 steps: 164.3371569162351\n",
      "Error in Q after 138000 steps: 163.98120593149895\n",
      "Error in Q after 139000 steps: 163.64730415632278\n",
      "Error in Q after 140000 steps: 162.82387996370247\n",
      "Error in Q after 141000 steps: 162.0019396459346\n",
      "Error in Q after 142000 steps: 162.00706724716628\n",
      "Error in Q after 143000 steps: 160.91421788088024\n",
      "Error in Q after 144000 steps: 160.2146100201254\n",
      "Error in Q after 145000 steps: 159.94214731542397\n",
      "Error in Q after 146000 steps: 159.472191449098\n",
      "Error in Q after 147000 steps: 158.19060519389143\n",
      "Error in Q after 148000 steps: 156.84919135341218\n",
      "Error in Q after 149000 steps: 156.85319457395647\n",
      "Error in Q after 150000 steps: 155.16851552974583\n",
      "Error in Q after 151000 steps: 155.1643506843884\n",
      "Error in Q after 152000 steps: 155.16504221932615\n",
      "Error in Q after 153000 steps: 154.84983306091863\n",
      "Error in Q after 154000 steps: 154.8485957146507\n",
      "Error in Q after 155000 steps: 154.62071096629438\n",
      "Error in Q after 156000 steps: 154.60059531849936\n",
      "Error in Q after 157000 steps: 154.3654385081396\n",
      "Error in Q after 158000 steps: 154.0464347310454\n",
      "Error in Q after 159000 steps: 153.39629305445666\n",
      "Error in Q after 160000 steps: 153.30715647198934\n",
      "Error in Q after 161000 steps: 152.23025478655958\n",
      "Error in Q after 162000 steps: 152.19592067601\n",
      "Error in Q after 163000 steps: 151.66093698977843\n",
      "Error in Q after 164000 steps: 150.09538445175124\n",
      "Error in Q after 165000 steps: 149.50976468678198\n",
      "Error in Q after 166000 steps: 149.513425869935\n",
      "Error in Q after 167000 steps: 148.53416102319682\n",
      "Error in Q after 168000 steps: 148.49808245217432\n",
      "Error in Q after 169000 steps: 148.5102570140311\n",
      "Error in Q after 170000 steps: 148.12435640234722\n",
      "Error in Q after 171000 steps: 147.35528855466185\n",
      "Error in Q after 172000 steps: 145.9124129890442\n",
      "Error in Q after 173000 steps: 145.93575492475998\n",
      "Error in Q after 174000 steps: 145.94742932125726\n",
      "Error in Q after 175000 steps: 145.69589547287922\n",
      "Error in Q after 176000 steps: 145.31899185718174\n",
      "Error in Q after 177000 steps: 145.29849365713707\n",
      "Error in Q after 178000 steps: 141.02901467510952\n",
      "Error in Q after 179000 steps: 141.01424447029945\n",
      "Error in Q after 180000 steps: 140.5750493940368\n",
      "Error in Q after 181000 steps: 138.58090453476203\n",
      "Error in Q after 182000 steps: 138.43852904185295\n",
      "Error in Q after 183000 steps: 135.36253175128954\n",
      "Error in Q after 184000 steps: 134.92945058378683\n",
      "Error in Q after 185000 steps: 134.3739182502693\n",
      "Error in Q after 186000 steps: 132.76156238193946\n",
      "Error in Q after 187000 steps: 131.27330868323776\n",
      "Error in Q after 188000 steps: 131.25313629833488\n",
      "Error in Q after 189000 steps: 129.84296406568333\n",
      "Error in Q after 190000 steps: 129.85525396204142\n",
      "Error in Q after 191000 steps: 128.79802904256283\n",
      "Error in Q after 192000 steps: 128.81163032373357\n",
      "Error in Q after 193000 steps: 125.96443407782125\n",
      "Error in Q after 194000 steps: 125.27968085316839\n",
      "Error in Q after 195000 steps: 125.40520893849528\n",
      "Error in Q after 196000 steps: 125.3856701710131\n",
      "Error in Q after 197000 steps: 125.09177453007915\n",
      "Error in Q after 198000 steps: 124.54454510207228\n",
      "Error in Q after 199000 steps: 123.09245054995722\n",
      "Error in Q after 200000 steps: 122.89960739338507\n",
      "Error in Q after 201000 steps: 122.97567924013451\n",
      "Error in Q after 202000 steps: 122.94908036391934\n",
      "Error in Q after 203000 steps: 122.26051451360465\n",
      "Error in Q after 204000 steps: 121.97216618436553\n",
      "Error in Q after 205000 steps: 121.30037134371082\n",
      "Error in Q after 206000 steps: 121.6175180113233\n",
      "Error in Q after 207000 steps: 121.64003116396744\n",
      "Error in Q after 208000 steps: 121.10792629450832\n",
      "Error in Q after 209000 steps: 119.13022218669504\n",
      "Error in Q after 210000 steps: 118.72543085018602\n",
      "Error in Q after 211000 steps: 117.13233814665193\n",
      "Error in Q after 212000 steps: 115.49796731055802\n",
      "Error in Q after 213000 steps: 115.60789887356428\n",
      "Error in Q after 214000 steps: 113.48357288596141\n",
      "Error in Q after 215000 steps: 113.49834063901832\n",
      "Error in Q after 216000 steps: 113.29796268604734\n",
      "Error in Q after 217000 steps: 113.22445356406301\n",
      "Error in Q after 218000 steps: 113.09448399131665\n",
      "Error in Q after 219000 steps: 113.11492500633454\n",
      "Error in Q after 220000 steps: 112.96423145887098\n",
      "Error in Q after 221000 steps: 112.87736772146832\n",
      "Error in Q after 222000 steps: 112.82867437328845\n",
      "Error in Q after 223000 steps: 112.39391886951273\n",
      "Error in Q after 224000 steps: 111.70885568319349\n",
      "Error in Q after 225000 steps: 111.37478420723245\n",
      "Error in Q after 226000 steps: 111.68381018927968\n",
      "Error in Q after 227000 steps: 111.11610281398723\n",
      "Error in Q after 228000 steps: 110.53657158116455\n",
      "Error in Q after 229000 steps: 108.42822062709632\n",
      "Error in Q after 230000 steps: 107.48985274311687\n",
      "Error in Q after 231000 steps: 107.1555395356859\n",
      "Error in Q after 232000 steps: 106.73230215429228\n",
      "Error in Q after 233000 steps: 106.76331385520801\n",
      "Error in Q after 234000 steps: 106.77282910251783\n",
      "Error in Q after 235000 steps: 106.52557907751162\n",
      "Error in Q after 236000 steps: 106.583984334719\n",
      "Error in Q after 237000 steps: 106.67438111857935\n",
      "Error in Q after 238000 steps: 106.55098522191224\n",
      "Error in Q after 239000 steps: 106.45854198438018\n",
      "Error in Q after 240000 steps: 106.21554200201436\n",
      "Error in Q after 241000 steps: 105.16151825592677\n",
      "Error in Q after 242000 steps: 102.38865554996654\n",
      "Error in Q after 243000 steps: 102.39071887624591\n",
      "Error in Q after 244000 steps: 101.38285803165357\n",
      "Error in Q after 245000 steps: 100.96048285402568\n",
      "Error in Q after 246000 steps: 100.42487074495926\n",
      "Error in Q after 247000 steps: 99.73859084562581\n",
      "Error in Q after 248000 steps: 99.6282453987433\n",
      "Error in Q after 249000 steps: 98.91907981160438\n",
      "Error in Q after 250000 steps: 97.10769876328204\n",
      "Error in Q after 251000 steps: 97.12390526253817\n",
      "Error in Q after 252000 steps: 97.34070336189218\n",
      "Error in Q after 253000 steps: 97.36242229728641\n",
      "Error in Q after 254000 steps: 97.67970381585911\n",
      "Error in Q after 255000 steps: 97.9015541356639\n",
      "Error in Q after 256000 steps: 97.97901129746579\n",
      "Error in Q after 257000 steps: 97.98211194699365\n",
      "Error in Q after 258000 steps: 98.3207499962732\n",
      "Error in Q after 259000 steps: 98.05789346195256\n",
      "Error in Q after 260000 steps: 98.00386089333588\n",
      "Error in Q after 261000 steps: 97.56394466991036\n",
      "Error in Q after 262000 steps: 97.48797996245497\n",
      "Error in Q after 263000 steps: 97.49435275213708\n",
      "Error in Q after 264000 steps: 97.76393761842793\n",
      "Error in Q after 265000 steps: 97.28162608770626\n",
      "Error in Q after 266000 steps: 97.39962816255529\n",
      "Error in Q after 267000 steps: 96.86673899318203\n",
      "Error in Q after 268000 steps: 96.43228894186132\n",
      "Error in Q after 269000 steps: 96.4642817186003\n",
      "Error in Q after 270000 steps: 95.77223890368548\n",
      "Error in Q after 271000 steps: 96.1946203290034\n",
      "Error in Q after 272000 steps: 95.95416534632975\n",
      "Error in Q after 273000 steps: 96.21977475244786\n",
      "Error in Q after 274000 steps: 96.29243678532721\n",
      "Error in Q after 275000 steps: 95.67403253670314\n",
      "Error in Q after 276000 steps: 95.68059831652849\n",
      "Error in Q after 277000 steps: 95.18164468581868\n",
      "Error in Q after 278000 steps: 95.18180705884838\n",
      "Error in Q after 279000 steps: 95.27131832198162\n",
      "Error in Q after 280000 steps: 95.59976511678977\n",
      "Error in Q after 281000 steps: 95.2370236129328\n",
      "Error in Q after 282000 steps: 95.2510581770378\n",
      "Error in Q after 283000 steps: 95.26110311010937\n",
      "Error in Q after 284000 steps: 95.31690237836375\n",
      "Error in Q after 285000 steps: 95.43758419022873\n",
      "Error in Q after 286000 steps: 95.33415162579556\n",
      "Error in Q after 287000 steps: 95.44363581207126\n",
      "Error in Q after 288000 steps: 95.97650359771292\n",
      "Error in Q after 289000 steps: 95.86434896100272\n",
      "Error in Q after 290000 steps: 95.78906299988402\n",
      "Error in Q after 291000 steps: 94.51750517179025\n",
      "Error in Q after 292000 steps: 94.54147004060012\n",
      "Error in Q after 293000 steps: 92.80999023659123\n",
      "Error in Q after 294000 steps: 92.85033152843283\n",
      "Error in Q after 295000 steps: 92.69388409946365\n",
      "Error in Q after 296000 steps: 92.07974734562006\n",
      "Error in Q after 297000 steps: 92.68843704602922\n",
      "Error in Q after 298000 steps: 92.6350717571932\n",
      "Error in Q after 299000 steps: 92.31369750925755\n",
      "Error in Q after 300000 steps: 92.02300839791712\n",
      "Error in Q after 301000 steps: 92.26623786605326\n",
      "Error in Q after 302000 steps: 92.25264012165114\n",
      "Error in Q after 303000 steps: 91.69613932091616\n",
      "Error in Q after 304000 steps: 91.73159175768534\n",
      "Error in Q after 305000 steps: 92.05720495628763\n",
      "Error in Q after 306000 steps: 92.64862184343096\n",
      "Error in Q after 307000 steps: 92.67530727925265\n",
      "Error in Q after 308000 steps: 92.25229958272048\n",
      "Error in Q after 309000 steps: 92.05523795911755\n",
      "Error in Q after 310000 steps: 92.059203810786\n",
      "Error in Q after 311000 steps: 92.11247815379356\n",
      "Error in Q after 312000 steps: 90.73636455760584\n",
      "Error in Q after 313000 steps: 90.77363092463007\n",
      "Error in Q after 314000 steps: 91.34933913555501\n",
      "Error in Q after 315000 steps: 90.42476131199126\n",
      "Error in Q after 316000 steps: 89.2060553800386\n",
      "Error in Q after 317000 steps: 89.23221029068947\n",
      "Error in Q after 318000 steps: 89.27357770892559\n",
      "Error in Q after 319000 steps: 89.28497138206207\n",
      "Error in Q after 320000 steps: 89.1037280474383\n",
      "Error in Q after 321000 steps: 89.1386414128899\n",
      "Error in Q after 322000 steps: 89.33637812634392\n",
      "Error in Q after 323000 steps: 89.40439684911136\n",
      "Error in Q after 324000 steps: 88.9249456837931\n",
      "Error in Q after 325000 steps: 89.02585327321282\n",
      "Error in Q after 326000 steps: 89.00959994687577\n",
      "Error in Q after 327000 steps: 88.85779776867817\n",
      "Error in Q after 328000 steps: 87.82082610022452\n",
      "Error in Q after 329000 steps: 86.87285480452293\n",
      "Error in Q after 330000 steps: 87.36599218770333\n",
      "Error in Q after 331000 steps: 87.39165618952033\n",
      "Error in Q after 332000 steps: 86.76762332655898\n",
      "Error in Q after 333000 steps: 86.77088151300667\n",
      "Error in Q after 334000 steps: 86.82978928280646\n",
      "Error in Q after 335000 steps: 86.92208013070928\n",
      "Error in Q after 336000 steps: 86.47399001043499\n",
      "Error in Q after 337000 steps: 87.23548359573456\n",
      "Error in Q after 338000 steps: 87.08096002583433\n",
      "Error in Q after 339000 steps: 87.08962338220235\n",
      "Error in Q after 340000 steps: 87.09128719469936\n",
      "Error in Q after 341000 steps: 87.24219132751212\n",
      "Error in Q after 342000 steps: 86.94599354331272\n",
      "Error in Q after 343000 steps: 87.00146274265492\n",
      "Error in Q after 344000 steps: 87.121574428167\n",
      "Error in Q after 345000 steps: 87.22467723068826\n",
      "Error in Q after 346000 steps: 87.30949159351418\n",
      "Error in Q after 347000 steps: 88.30878588555503\n",
      "Error in Q after 348000 steps: 88.20448758791373\n",
      "Error in Q after 349000 steps: 88.44236977902541\n",
      "Error in Q after 350000 steps: 89.06655129268178\n",
      "Error in Q after 351000 steps: 89.06749769665471\n",
      "Error in Q after 352000 steps: 88.72799769807561\n",
      "Error in Q after 353000 steps: 88.81447009305123\n",
      "Error in Q after 354000 steps: 89.09667225546342\n",
      "Error in Q after 355000 steps: 88.81516797306108\n",
      "Error in Q after 356000 steps: 88.95168913626566\n",
      "Error in Q after 357000 steps: 89.01295126529395\n",
      "Error in Q after 358000 steps: 88.36569576044602\n",
      "Error in Q after 359000 steps: 88.4157376261615\n",
      "Error in Q after 360000 steps: 88.06834991045133\n",
      "Error in Q after 361000 steps: 88.13986258855606\n",
      "Error in Q after 362000 steps: 88.25812321484383\n",
      "Error in Q after 363000 steps: 87.40688900021735\n",
      "Error in Q after 364000 steps: 87.50474258497003\n",
      "Error in Q after 365000 steps: 86.29995809156803\n",
      "Error in Q after 366000 steps: 86.32339004991816\n",
      "Error in Q after 367000 steps: 87.38298144201845\n",
      "Error in Q after 368000 steps: 87.46251998430371\n",
      "Error in Q after 369000 steps: 87.3233900141714\n",
      "Error in Q after 370000 steps: 87.56188876616498\n",
      "Error in Q after 371000 steps: 87.65931045948457\n",
      "Error in Q after 372000 steps: 88.27604786810774\n",
      "Error in Q after 373000 steps: 88.10570795494583\n",
      "Error in Q after 374000 steps: 88.6419920299335\n",
      "Error in Q after 375000 steps: 88.78022234978192\n",
      "Error in Q after 376000 steps: 88.9167306593815\n",
      "Error in Q after 377000 steps: 88.63841042243331\n",
      "Error in Q after 378000 steps: 89.13760497255383\n",
      "Error in Q after 379000 steps: 89.24459358855262\n",
      "Error in Q after 380000 steps: 89.25339879309368\n",
      "Error in Q after 381000 steps: 89.5248503478526\n",
      "Error in Q after 382000 steps: 89.53908082056662\n",
      "Error in Q after 383000 steps: 89.64760057473663\n",
      "Error in Q after 384000 steps: 89.91793077284316\n",
      "Error in Q after 385000 steps: 89.94225904540265\n",
      "Error in Q after 386000 steps: 89.84535071553827\n",
      "Error in Q after 387000 steps: 89.87104516739636\n",
      "Error in Q after 388000 steps: 89.91781969826981\n",
      "Error in Q after 389000 steps: 90.08122458986216\n",
      "Error in Q after 390000 steps: 90.42737872432177\n",
      "Error in Q after 391000 steps: 90.31046705033683\n",
      "Error in Q after 392000 steps: 90.43150793914855\n",
      "Error in Q after 393000 steps: 90.40482717349482\n",
      "Error in Q after 394000 steps: 90.40173819969031\n",
      "Error in Q after 395000 steps: 90.3206113445356\n",
      "Error in Q after 396000 steps: 90.9955719455438\n",
      "Error in Q after 397000 steps: 91.09076926726466\n",
      "Error in Q after 398000 steps: 91.09076926726466\n",
      "Error in Q after 399000 steps: 92.04490795287073\n",
      "Error in Q after 400000 steps: 92.60179468490774\n",
      "Error in Q after 401000 steps: 92.62507006501123\n",
      "Error in Q after 402000 steps: 92.54918127715301\n",
      "Error in Q after 403000 steps: 92.56992984589948\n",
      "Error in Q after 404000 steps: 92.57804249348933\n",
      "Error in Q after 405000 steps: 92.67352813793143\n",
      "Error in Q after 406000 steps: 92.88267182121075\n",
      "Error in Q after 407000 steps: 93.0098325353977\n",
      "Error in Q after 408000 steps: 94.08679349194081\n",
      "Error in Q after 409000 steps: 95.50324710883542\n",
      "Error in Q after 410000 steps: 95.55578774679998\n",
      "Error in Q after 411000 steps: 95.71309602400905\n",
      "Error in Q after 412000 steps: 95.97445286606617\n",
      "Error in Q after 413000 steps: 96.22604563589083\n",
      "Error in Q after 414000 steps: 96.30271767022602\n",
      "Error in Q after 415000 steps: 95.96546928642529\n",
      "Error in Q after 416000 steps: 96.05410392498253\n",
      "Error in Q after 417000 steps: 96.10453137200354\n",
      "Error in Q after 418000 steps: 96.38688337080765\n",
      "Error in Q after 419000 steps: 96.61873481364754\n",
      "Error in Q after 420000 steps: 96.70038040964685\n",
      "Error in Q after 421000 steps: 96.88797813028086\n",
      "Error in Q after 422000 steps: 96.92770148693997\n",
      "Error in Q after 423000 steps: 96.99979347817842\n",
      "Error in Q after 424000 steps: 97.92840718222655\n",
      "Error in Q after 425000 steps: 98.11721901017857\n",
      "Error in Q after 426000 steps: 98.1602229382604\n",
      "Error in Q after 427000 steps: 98.1745577017124\n",
      "Error in Q after 428000 steps: 98.21806945798376\n",
      "Error in Q after 429000 steps: 99.11624370458148\n",
      "Error in Q after 430000 steps: 99.13706683292953\n",
      "Error in Q after 431000 steps: 99.75001252173816\n",
      "Error in Q after 432000 steps: 99.92879473720716\n",
      "Error in Q after 433000 steps: 99.97813423519042\n",
      "Error in Q after 434000 steps: 100.09236310869002\n",
      "Error in Q after 435000 steps: 100.58730825188943\n",
      "Error in Q after 436000 steps: 100.81507493904341\n",
      "Error in Q after 437000 steps: 100.81324651700741\n",
      "Error in Q after 438000 steps: 100.8969790883782\n",
      "Error in Q after 439000 steps: 101.12408571265831\n",
      "Error in Q after 440000 steps: 101.23508836961979\n",
      "Error in Q after 441000 steps: 101.37656387119455\n",
      "Error in Q after 442000 steps: 101.41792758068702\n",
      "Error in Q after 443000 steps: 101.4334770592183\n",
      "Error in Q after 444000 steps: 101.4933068907602\n",
      "Error in Q after 445000 steps: 102.13418399906783\n",
      "Error in Q after 446000 steps: 102.23019836089875\n",
      "Error in Q after 447000 steps: 103.23367021136168\n",
      "Error in Q after 448000 steps: 103.3356161771095\n",
      "Error in Q after 449000 steps: 103.35078057799055\n",
      "Error in Q after 450000 steps: 103.39875597314519\n",
      "Error in Q after 451000 steps: 103.46833180714542\n",
      "Error in Q after 452000 steps: 103.51770051133222\n",
      "Error in Q after 453000 steps: 103.33794288561752\n",
      "Error in Q after 454000 steps: 103.35583800461696\n",
      "Error in Q after 455000 steps: 103.64152129077999\n",
      "Error in Q after 456000 steps: 103.8996285560925\n",
      "Error in Q after 457000 steps: 104.4247976526203\n",
      "Error in Q after 458000 steps: 105.82875849028285\n",
      "Error in Q after 459000 steps: 105.88129908564207\n",
      "Error in Q after 460000 steps: 106.27673095952433\n",
      "Error in Q after 461000 steps: 106.29078732120175\n",
      "Error in Q after 462000 steps: 106.41235364283406\n",
      "Error in Q after 463000 steps: 106.51129321382945\n",
      "Error in Q after 464000 steps: 106.92399041475038\n",
      "Error in Q after 465000 steps: 107.16579783514673\n",
      "Error in Q after 466000 steps: 106.86965817183244\n",
      "Error in Q after 467000 steps: 106.93722756132061\n",
      "Error in Q after 468000 steps: 107.24746765988247\n",
      "Error in Q after 469000 steps: 107.23078440550955\n",
      "Error in Q after 470000 steps: 107.34408083469064\n",
      "Error in Q after 471000 steps: 107.45191045058147\n",
      "Error in Q after 472000 steps: 107.64661705861512\n",
      "Error in Q after 473000 steps: 107.78436608251921\n",
      "Error in Q after 474000 steps: 108.42139299989526\n",
      "Error in Q after 475000 steps: 108.46456705080057\n",
      "Error in Q after 476000 steps: 108.88489306135007\n",
      "Error in Q after 477000 steps: 108.79653532132492\n",
      "Error in Q after 478000 steps: 109.05422672383494\n",
      "Error in Q after 479000 steps: 109.13804423859341\n",
      "Error in Q after 480000 steps: 109.38515644973822\n",
      "Error in Q after 481000 steps: 110.53555192579309\n",
      "Error in Q after 482000 steps: 110.8171663315376\n",
      "Error in Q after 483000 steps: 111.09138006195096\n",
      "Error in Q after 484000 steps: 111.15355876994617\n",
      "Error in Q after 485000 steps: 111.92611399868122\n",
      "Error in Q after 486000 steps: 112.04662758022461\n",
      "Error in Q after 487000 steps: 112.71336882604403\n",
      "Error in Q after 488000 steps: 112.94247381104375\n",
      "Error in Q after 489000 steps: 113.24426001183271\n",
      "Error in Q after 490000 steps: 113.31045259051709\n",
      "Error in Q after 491000 steps: 113.31465456498142\n",
      "Error in Q after 492000 steps: 113.31410638171131\n",
      "Error in Q after 493000 steps: 113.6019364405401\n",
      "Error in Q after 494000 steps: 113.78312735395745\n",
      "Error in Q after 495000 steps: 114.12014640718934\n",
      "Error in Q after 496000 steps: 114.3112246762259\n",
      "Error in Q after 497000 steps: 113.9759266706809\n",
      "Error in Q after 498000 steps: 114.15552089717534\n",
      "Error in Q after 499000 steps: 114.23385393705925\n",
      "Error in Q after 500000 steps: 114.25730896292309\n"
     ]
    }
   ],
   "source": [
    "def qlearning(mdp: tuple, n: int, qinit: np.ndarray):\n",
    "    X, A, _, _, gamma = mdp\n",
    "\n",
    "    Q = qinit.copy()\n",
    "    \n",
    "    x = rnd.randint(len(X))\n",
    "    \n",
    "    for _ in range(n):\n",
    "        \n",
    "        a = egreedy(Q[x], eps=0.15)\n",
    "        \n",
    "        x, a, cost, x_prime = sample_transition(mdp, x, a)\n",
    "        \n",
    "        alpha = 0.3\n",
    "        Q[x, a] = Q[x, a] + alpha * (cost + gamma * np.max(Q[x_prime]) - Q[x, a])\n",
    "        \n",
    "        x = x_prime\n",
    "    \n",
    "    return Q\n",
    "\n",
    "\n",
    "rnd.seed(42)\n",
    "\n",
    "# Initialize Q-function\n",
    "qinit = np.zeros((len(M[0]), len(M[1])))\n",
    "\n",
    "# Run 1000 steps of model-based learning\n",
    "qnew = qlearning(M, 1000, qinit)\n",
    "\n",
    "# Compare the learned Q with the optimal Q\n",
    "print('Error in Q after 1000 steps:', np.linalg.norm(qnew - Qopt))\n",
    "\n",
    "# Run 1000 additional steps of model-based learning\n",
    "qnew = qlearning(M, 1000, qnew)\n",
    "\n",
    "for i in range(500):\n",
    "    # Compare the learned Q with the optimal Q\n",
    "    print('Error in Q after',(i+1) * 1000,'steps:', np.linalg.norm(qnew - Qopt))\n",
    "\n",
    "    # Run 1000 additional steps of model-based learning\n",
    "    qnew = qlearning(M, 5000, qnew)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Error in Q after 1000 steps: 388.4431476570159\n",
    "Error in Q after 2000 steps: 388.0174458876989\n",
    "Error in Q after 3000 steps: 387.4375098536993\n",
    "Error in Q after 4000 steps: 386.78717191665015\n",
    "Error in Q after 5000 steps: 382.97135539410607\n",
    "Error in Q after 6000 steps: 379.29816388372336"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T09:18:48.567188Z",
     "start_time": "2019-12-09T09:18:48.333226Z"
    },
    "scrolled": false
   },
   "source": [
    "As an example using the taxi MDP, we could run:\n",
    "\n",
    "```python\n",
    "rnd.seed(42)\n",
    "\n",
    "# Initialize Q-function\n",
    "qinit = np.zeros((len(M[0]), len(M[1])))\n",
    "\n",
    "# Run 1000 steps of model-based learning\n",
    "qnew = qlearning(M, 1000, qinit)\n",
    "\n",
    "# Compare the learned Q with the optimal Q\n",
    "print('Error in Q after 1000 steps:', np.linalg.norm(qnew - Qopt))\n",
    "\n",
    "# Run 1000 additional steps of model-based learning\n",
    "qnew = qlearning(M, 1000, qnew)\n",
    "\n",
    "# Compare once again the learned Q with the optimal Q\n",
    "print('Error in Q after 2000 steps:', np.linalg.norm(qnew - Qopt))\n",
    "```\n",
    "\n",
    "to get\n",
    "\n",
    "```\n",
    "Error in Q after 1000 steps: 389.72801262556976\n",
    "Error in Q after 2000 steps: 389.7261836534418\n",
    "```\n",
    "\n",
    "Once again, even if the seed is fixed, the numerical values may differ somewhat from those above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Activity 5. \n",
    "\n",
    "Write a function `sarsa` that implements the SARSA algorithm discussed in class. Your function should receive as input arguments \n",
    "\n",
    "* A tuple, `mdp`, containing the description of an **arbitrary** MDP. The structure of the tuple is similar to that provided in the examples above. \n",
    "* An integer, `n`, corresponding he number of steps that your algorithm should run.\n",
    "*  A `numpy` array `qinit` with as many rows as the number of states in `mdp` and as many columns as the number of actions in `mdp`. The matrix `qinit` should be used to initialize the $Q$-function being learned by your function.\n",
    "\n",
    "Your function should simulate an interaction of `n` steps between the agent and the environment, starting from an arbitrary state chosen uniformly at random, and during which it should perform `n` iterations of the SARSA algorithm seen in class. The $Q$-function to be learned should be initialized using `qinit`. \n",
    "\n",
    "Note that, at each step of the interaction,\n",
    "\n",
    "* The agent should observe the current state, and select an action using an $\\epsilon$-greedy policy with respect to its current estimate of the optimal $Q$-values. You should use the function `egreedy` from Activity 2, with $\\epsilon=0.15$. **Do not adjust the value of $\\epsilon$ during learning.**\n",
    "* Given the state and action, you must then compute the cost and generate the next state, using `mdp` and the function `sample_transition` from Activity 1.\n",
    "* With this transition information (state, action, cost, next-state), you can now perform an update to the $Q$-function.\n",
    "* When updating the components $(x,a)$ of the model, use the step-size $\\alpha=0.3$.\n",
    "\n",
    "Your function should return a `numpy` array with as many rows as the number of states in `mdp` and as many columns as the number of actions in `mdp`, corresponding to the learned $Q$-function.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T09:18:48.771464Z",
     "start_time": "2019-12-09T09:18:48.766170Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in Q after 1000 steps: 390.0723574051231\n",
      "Error in Q after 2000 steps: 389.97599666017805\n"
     ]
    }
   ],
   "source": [
    "def sarsa(mdp:tuple, n:int, qinit:np.array):\n",
    "    X, A, P, c, gamma = mdp\n",
    "    n_states = len(X)\n",
    "    x = rnd.randint(n_states)\n",
    "    Q = qinit\n",
    "    a = egreedy(Q[x], 0.15)\n",
    "\n",
    "    for _ in range(n):\n",
    "        _, _, cost, x_new = sample_transition(mdp, x, a)\n",
    "        a_new = egreedy(Q[x_new], 0.15)\n",
    "        Q[x][a] = Q[x][a] + 0.3 * (cost + gamma * Q[x_new][a_new] - Q[x][a])\n",
    "        x = x_new\n",
    "        a = a_new\n",
    "    return Q\n",
    "\n",
    "\n",
    "rnd.seed(42)\n",
    "\n",
    "# Initialize Q-function\n",
    "qinit = np.zeros((len(M[0]), len(M[1])))\n",
    "\n",
    "# Run 1000 steps of model-based learning\n",
    "qnew = sarsa(M, 1000, qinit)\n",
    "\n",
    "# Compare the learned Q with the optimal Q\n",
    "print('Error in Q after 1000 steps:', np.linalg.norm(qnew - Qopt))\n",
    "\n",
    "# Run 1000 additional steps of model-based learning\n",
    "qnew = sarsa(M, 1000, qnew)\n",
    "\n",
    "# Compare once again the learned Q with the optimal Q\n",
    "print('Error in Q after 2000 steps:', np.linalg.norm(qnew - Qopt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T09:18:48.567188Z",
     "start_time": "2019-12-09T09:18:48.333226Z"
    },
    "scrolled": false
   },
   "source": [
    "As an example using the taxi MDP, we could run:\n",
    "\n",
    "```python\n",
    "rnd.seed(42)\n",
    "\n",
    "# Initialize Q-function\n",
    "qinit = np.zeros((len(M[0]), len(M[1])))\n",
    "\n",
    "# Run 1000 steps of model-based learning\n",
    "qnew = sarsa(M, 1000, qinit)\n",
    "\n",
    "# Compare the learned Q with the optimal Q\n",
    "print('Error in Q after 1000 steps:', np.linalg.norm(qnew - Qopt))\n",
    "\n",
    "# Run 1000 additional steps of model-based learning\n",
    "qnew = sarsa(M, 1000, qnew)\n",
    "\n",
    "# Compare once again the learned Q with the optimal Q\n",
    "print('Error in Q after 2000 steps:', np.linalg.norm(qnew - Qopt))\n",
    "```\n",
    "\n",
    "to get\n",
    "\n",
    "```\n",
    "Error in Q after 1000 steps: 388.44986735098877\n",
    "Error in Q after 2000 steps: 387.75181889394776\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T09:19:53.521353Z",
     "start_time": "2019-12-09T09:18:48.932380Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " = Training (run n. 0) =\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 244/10000 [08:11<5:27:14,  2.01s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[69], line 40\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# Run evaluation\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m trange(ITERS):\n\u001b[1;32m---> 40\u001b[0m     qmb, pmb, cmb \u001b[38;5;241m=\u001b[39m \u001b[43mmb_learning\u001b[49m\u001b[43m(\u001b[49m\u001b[43mM\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSTEPS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqmb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpmb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcmb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     41\u001b[0m     Emb[t \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mnorm(Qopt \u001b[38;5;241m-\u001b[39m qmb)\n\u001b[0;32m     43\u001b[0m     qql \u001b[38;5;241m=\u001b[39m qlearning(M, STEPS, qql)\n",
      "Cell \u001b[1;32mIn[65], line 21\u001b[0m, in \u001b[0;36mmb_learning\u001b[1;34m(mdp, n, qinit, Pinit, cinit)\u001b[0m\n\u001b[0;32m     17\u001b[0m c \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m alpha \u001b[38;5;241m*\u001b[39m (cost \u001b[38;5;241m-\u001b[39m c) \n\u001b[0;32m     19\u001b[0m N_t[x][a] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m---> 21\u001b[0m Q_next_min \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mQ\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m Q_next \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(P[a], Q_next_min)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Update Q-values using the provided equation\u001b[39;00m\n",
      "File \u001b[1;32m<__array_function__ internals>:177\u001b[0m, in \u001b[0;36mamin\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import trange\n",
    "\n",
    "STEPS = 100\n",
    "ITERS = 10000\n",
    "RUNS  = 5\n",
    "\n",
    "iters = range(0, STEPS * ITERS + 1, STEPS)\n",
    "\n",
    "# Error matrices\n",
    "Emb = np.zeros(ITERS + 1)\n",
    "Eql = np.zeros(ITERS + 1)\n",
    "Ess = np.zeros(ITERS + 1)\n",
    "\n",
    "Emb[0] = np.linalg.norm(Qopt) * RUNS\n",
    "Eql[0] = Emb[0]\n",
    "Ess[0] = Emb[0]\n",
    "\n",
    "rnd.seed(42)\n",
    "\n",
    "for n in range(RUNS):\n",
    "    \n",
    "    print('\\n = Training (run n. %i) =' % n)\n",
    "\n",
    "    # Initialization\n",
    "    pmb = ()\n",
    "    for a in range(len(M[1])):\n",
    "        pmb += (np.eye(len(M[0])),)\n",
    "    cmb = np.zeros((len(M[0]), len(M[1])))\n",
    "    qmb = np.zeros((len(M[0]), len(M[1])))\n",
    "\n",
    "    qql = np.zeros((len(M[0]), len(M[1])))\n",
    "\n",
    "    qss = np.zeros((len(M[0]), len(M[1])))\n",
    "\n",
    "    # Run evaluation\n",
    "    for t in trange(ITERS):\n",
    "        qmb, pmb, cmb = mb_learning(M, STEPS, qmb, pmb, cmb)\n",
    "        Emb[t + 1] += np.linalg.norm(Qopt - qmb)\n",
    "\n",
    "        qql = qlearning(M, STEPS, qql)\n",
    "        Eql[t + 1] += np.linalg.norm(Qopt - qql)\n",
    "\n",
    "        qss = sarsa(M, STEPS, qss)\n",
    "        Ess[t + 1] += np.linalg.norm(Qopt - qss)\n",
    "        \n",
    "Emb /= RUNS\n",
    "Eql /= RUNS\n",
    "Ess /= RUNS\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(iters, Emb, label='Model based learning')\n",
    "plt.plot(iters, Eql, label='Q-learning')\n",
    "plt.plot(iters, Ess, label='SARSA')\n",
    "plt.legend()\n",
    "plt.xlabel('N. iterations')\n",
    "plt.ylabel('Error in $Q$-function')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also run the following code, to compare the performance of the three methods:\n",
    "\n",
    "```python\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import trange\n",
    "\n",
    "STEPS = 100\n",
    "ITERS = 10000\n",
    "RUNS  = 5\n",
    "\n",
    "iters = range(0, STEPS * ITERS + 1, STEPS)\n",
    "\n",
    "# Error matrices\n",
    "Emb = np.zeros(ITERS + 1)\n",
    "Eql = np.zeros(ITERS + 1)\n",
    "Ess = np.zeros(ITERS + 1)\n",
    "\n",
    "Emb[0] = np.linalg.norm(Qopt) * RUNS\n",
    "Eql[0] = Emb[0]\n",
    "Ess[0] = Emb[0]\n",
    "\n",
    "rnd.seed(42)\n",
    "\n",
    "for n in range(RUNS):\n",
    "    \n",
    "    print('\\n = Training (run n. %i) =' % n)\n",
    "\n",
    "    # Initialization\n",
    "    pmb = ()\n",
    "    for a in range(len(M[1])):\n",
    "        pmb += (np.eye(len(M[0])),)\n",
    "    cmb = np.zeros((len(M[0]), len(M[1])))\n",
    "    qmb = np.zeros((len(M[0]), len(M[1])))\n",
    "\n",
    "    qql = np.zeros((len(M[0]), len(M[1])))\n",
    "\n",
    "    qss = np.zeros((len(M[0]), len(M[1])))\n",
    "\n",
    "    # Run evaluation\n",
    "    for t in trange(ITERS):\n",
    "        qmb, pmb, cmb = mb_learning(M, STEPS, qmb, pmb, cmb)\n",
    "        Emb[t + 1] += np.linalg.norm(Qopt - qmb)\n",
    "\n",
    "        qql = qlearning(M, STEPS, qql)\n",
    "        Eql[t + 1] += np.linalg.norm(Qopt - qql)\n",
    "\n",
    "        qss = sarsa(M, STEPS, qss)\n",
    "        Ess[t + 1] += np.linalg.norm(Qopt - qss)\n",
    "        \n",
    "Emb /= RUNS\n",
    "Eql /= RUNS\n",
    "Ess /= RUNS\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(iters, Emb, label='Model based learning')\n",
    "plt.plot(iters, Eql, label='Q-learning')\n",
    "plt.plot(iters, Ess, label='SARSA')\n",
    "plt.legend()\n",
    "plt.xlabel('N. iterations')\n",
    "plt.ylabel('Error in $Q$-function')\n",
    "plt.tight_layout()\n",
    "```\n",
    "\n",
    "**Note:** The code above takes a while to conclude. If you want to observe faster results, you may try with a single run (set `RUNS = 1` above) or decrease the training time (changing `ITERS` above). However, the plot you will obtain will differ from the one provided.\n",
    "\n",
    "As the output, you should observe a plot similar to the one below.\n",
    "\n",
    "<img src=\"plot.png\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Activity 6.\n",
    "\n",
    "**Based on the results you obtained when running the above code with your algorithms**, discuss the differences observed between the performance of the three methods.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "SARSA (on-policy)\n",
    "Obtém menores erros mais rapidamente, mas não converge exatamente para a politica optima.\n",
    "Toma uma rota mais segura devido à probablididade, epsilon, de escolher um movimento aleatório, que o pode levar a cair do penhasco, como representado no caso do cliff walking.\n",
    "\n",
    "Pode convergir para politica otima se o valor de epsilon for diminuindo ao longo das iterações (faz de cada vez menos random moves)\n",
    "\n",
    "Q-learning (off-policy)\n",
    "Necessita de mais iteracoes para convergir, mas converge para a politica otima diretamente.\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
