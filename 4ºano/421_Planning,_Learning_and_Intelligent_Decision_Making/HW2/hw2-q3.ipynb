{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-15T17:36:01.123055Z","iopub.status.busy":"2023-12-15T17:36:01.122784Z","iopub.status.idle":"2023-12-15T17:36:31.454465Z","shell.execute_reply":"2023-12-15T17:36:31.453410Z","shell.execute_reply.started":"2023-12-15T17:36:01.123030Z"},"id":"y8_zCYtOhLvO","outputId":"01955faf-ff72-4959-a362-53579ce02c4e","trusted":true},"outputs":[],"source":["!pip install ml_collections \"textdistance[extras]\""]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-15T17:36:31.456632Z","iopub.status.busy":"2023-12-15T17:36:31.456320Z","iopub.status.idle":"2023-12-15T17:36:31.477898Z","shell.execute_reply":"2023-12-15T17:36:31.476971Z","shell.execute_reply.started":"2023-12-15T17:36:31.456604Z"},"id":"vSDn-u-xYZma","trusted":true},"outputs":[],"source":["# hw2/utils.py\n","\n","import enum\n","import pickle\n","\n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","\n","def save_obj(obj, path):\n","    with open(path, \"wb\") as f:\n","        pickle.dump(obj, f)\n","\n","\n","def save_txt(str, path):\n","    with open(path, \"w\", encoding=\"utf-8\") as f:\n","        f.write(str)\n","\n","\n","def load_metrics(file_path):\n","    with open(file_path, \"rb\") as f:\n","        metrics = pickle.load(f)\n","\n","    return metrics\n","\n","\n","def plot_metrics(metrics, plot_path, phase=\"train\"):\n","    metric_keys = list(metrics[0][phase].keys())\n","    len_x = len(metrics)\n","    x_label = \"epoch\".title()\n","    x = np.arange(len_x)\n","\n","    for k in metric_keys:\n","        if k == \"n\":\n","            continue\n","\n","        metric_path = plot_path / f\"{k}__{phase}.pdf\"\n","        y = np.array([metrics[i][phase][k] for i in range(len_x)])\n","\n","        if k != \"loss\":\n","            batch = np.array([metrics[i][phase][\"n\"] for i in range(len_x)])\n","            y /= batch\n","\n","        y = y.mean(axis=-1)\n","        _create_plot([y], [x], x_label, k.title(), metric_path, show=True)\n","\n","\n","def cmp_phase_metrics(metrics, key, phases, plot_path, labels):\n","    len_x = len(metrics)\n","    x_label = \"epoch\".title()\n","    x = np.arange(len_x)\n","    y = []\n","\n","    for phase in phases:\n","        y_phase = np.array([metrics[i][phase][key] for i in range(len_x)])\n","\n","        if key != \"loss\":\n","            batch = np.array([metrics[i][phase][\"n\"] for i in range(len_x)])\n","            y_phase /= batch\n","\n","        y_phase = y_phase.mean(axis=-1)\n","        y.append(y_phase)\n","\n","    x = [x] * len(y)\n","    fig_path = plot_path / f\"{key}__{'_'.join(phases)}.pdf\"\n","    labels = [labels[p] for p in phases]\n","    _create_plot(y, x, x_label, key.title(), fig_path, line_labels=labels, show=True)\n","\n","\n","def _create_plot(ys, xs, x_label, y_label, save_path, line_labels=None, show=False):\n","    fig, ax = plt.subplots()\n","    for y, x in zip(ys, xs):\n","        ax.plot(x, y)\n","\n","    ax.set_xlabel(x_label)\n","    ax.set_ylabel(y_label)\n","    ax.grid()\n","    fig.savefig(save_path, format=\"pdf\", bbox_inches=\"tight\")\n","\n","    if line_labels is not None:\n","        ax.legend(line_labels)\n","\n","    if show:\n","        plt.show(fig)\n","\n","    plt.close(fig)\n","\n","\n","class Dataset(str, enum.Enum):\n","    LJSPEECH_STFT = \"ljspeech_stft\"\n","    LJSPEECH_MEL = \"ljspeech_mel\"\n","\n","\n","class DataSplit(str, enum.Enum):\n","    TRAIN = \"train\"\n","    VALIDATION = \"validation\"\n","    TEST = \"test\"\n","\n","\n","class AudioTransformType(str, enum.Enum):\n","    STFT = \"stft\"\n","    LOG_MEL_STFT = \"log_mel_stft\"\n","\n","\n","class EncoderType(str, enum.Enum):\n","    RNN = \"rnn\"\n","    TRANSFORMER = \"transformer\"\n","\n","\n","class DecoderType(str, enum.Enum):\n","    RNN = \"rnn\"\n","    TRANSFORMER = \"transformer\"\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-15T17:36:31.479932Z","iopub.status.busy":"2023-12-15T17:36:31.479673Z","iopub.status.idle":"2023-12-15T17:36:31.563241Z","shell.execute_reply":"2023-12-15T17:36:31.562577Z","shell.execute_reply.started":"2023-12-15T17:36:31.479907Z"},"id":"1OA5lnc_OWlc","trusted":true},"outputs":[],"source":["# hw2/config.py\n","\n","from ml_collections import config_dict\n","\n","\n","def get_config():\n","    config = config_dict.ConfigDict()\n","    config.seed = 42\n","    config.n_fft = 400\n","    config.n_vocab = 35\n","    config.text_len = 99\n","    config.audio_freq = 80  # number of mel filters\n","    config.audio_time = 2754  # 10 seconds of sound\n","    config.epochs = 15\n","    config.lr = 1e-3\n","    config.unk_token_idx = 3\n","    config.storage_folder = \"./storage\"\n","    config.speech_to_text = dict(\n","        max_text_len=config.get_ref(\"text_len\"),\n","        encoder_kwargs=dict(\n","            n_layers=2,\n","            freq_dim=config.get_ref(\"audio_freq\"),\n","            time_dim=config.get_ref(\"audio_time\") // 2,\n","            hidden_dim=200,\n","            n_heads=2,\n","            feed_fwd_dim=400,\n","        ),\n","        decoder_type=DecoderType.TRANSFORMER,\n","        decoder_kwargs=dict(\n","            freq_dim=config.get_ref(\"n_vocab\"),\n","            time_dim=config.get_ref(\"text_len\"),\n","            hidden_dim=200,\n","            n_heads=2,\n","            feed_fwd_dim=400,\n","        ),\n","    )\n","\n","    config.data = dict(\n","        data_folder=\"./data/\",\n","        data_split=dict(train=(0, 9825), validation=(9825, 10480), test=(10480, 13100)),\n","        dataloader=dict(\n","            train=dict(batch_size=64, shuffle=True, num_workers=2, prefetch_factor=2),\n","            validation=dict(\n","                batch_size=16, shuffle=False, num_workers=2, prefetch_factor=2\n","            ),\n","            test=dict(\n","                batch_size=16, shuffle=False, num_workers=2, prefetch_factor=2\n","            ),\n","        ),\n","        transforms=dict(\n","            audio=dict(\n","                spectrogram=dict(\n","                    n_fft=config.get_ref(\"n_fft\"),\n","                    hop_length=160,\n","                    power=2,\n","                    center=True,\n","                    normalized=False,\n","                    onesided=None,\n","                ),\n","                mel_scale=dict(\n","                    n_mels=config.get_ref(\"audio_freq\"),\n","                    n_stft=config.get_ref(\"n_fft\") // 2 + 1,\n","                    norm=\"slaney\",\n","                    mel_scale=\"slaney\",\n","                ),\n","                pad=dict(\n","                    l=0,\n","                    t=0,\n","                    r=config.get_ref(\"audio_time\"),\n","                    b=0,\n","                ),\n","                freq_masking=dict(freq_mask_param=27),\n","                time_masking=dict(time_mask_param=80),\n","            ),\n","            text=dict(\n","                token_vectorizer=dict(\n","                    max_len=config.get_ref(\"text_len\"),\n","                    start=\"<\",\n","                    stop=\">\",\n","                    empty=\"@\",\n","                    unk=\"#\",\n","                    n_vocab=config.get_ref(\"n_vocab\"),\n","                )\n","            ),\n","        ),\n","    )\n","\n","    return config.lock()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-15T17:36:31.565324Z","iopub.status.busy":"2023-12-15T17:36:31.565059Z","iopub.status.idle":"2023-12-15T17:36:33.511987Z","shell.execute_reply":"2023-12-15T17:36:33.511025Z","shell.execute_reply.started":"2023-12-15T17:36:31.565300Z"},"id":"pqcDO618mh4I","trusted":true},"outputs":[],"source":["# hw2/data.py\n","\n","import functools\n","import os\n","from typing import Callable, Dict, List, Tuple\n","\n","from ml_collections import config_dict\n","import torch\n","import torchaudio\n","import torchvision\n","\n","\n","class TokenVectorizer(torch.nn.Module):\n","    def __init__(\n","        self,\n","        n_vocab: int,\n","        max_len: int = 50,\n","        start: str = \"<\",\n","        stop: str = \">\",\n","        empty: str = \"@\",\n","        unk: str = \"#\",\n","    ) -> None:\n","        super().__init__()\n","        self._max_len = max_len\n","        self._start = start\n","        self._stop = stop\n","        self._empty = empty\n","        self._unk = unk\n","        self._vocab = [\" \", \",\", \".\", \"?\", \"-\"]\n","        self._vocab += [chr(i + 96) for i in range(1, 27)]\n","\n","        assert self._start not in self._vocab\n","        assert self._stop not in self._vocab\n","        assert self._empty not in self._vocab\n","        assert self._unk not in self._vocab\n","        self._vocab = [self._start, self._stop, self._empty, self._unk] + self._vocab\n","        assert len(self._vocab) == n_vocab\n","        self._unk_idx = 3\n","        self._mapper = {c: i for i, c in enumerate(self._vocab)}\n","        self._inv_mapper = {v: k for k, v in self._mapper.items()}\n","\n","    def forward(self, transcript: str) -> torch.Tensor:\n","        transcript = transcript.lower()[: self._max_len - 2]\n","        transcript = self._start + transcript + self._stop\n","        pad = [self._mapper[self._empty]] * (self._max_len - len(transcript))\n","        return torch.tensor(\n","            [self._mapper.get(c, self._unk_idx) for c in transcript] + pad\n","        )\n","\n","    @property\n","    def invert_mapper(self) -> Dict[int, str]:\n","        return self._inv_mapper\n","\n","    @property\n","    def mapper(self) -> Dict[str, int]:\n","        return self._mapper\n","\n","    @property\n","    def vocab(self) -> List[str]:\n","        return self._vocab\n","\n","\n","class SpeechRecognitionDataset(torch.utils.data.Dataset):\n","    def __init__(\n","        self,\n","        ds: torch.utils.data.Dataset,\n","        get_el: Callable,\n","        audio_transforms: torch.nn.Module,\n","        text_transforms: torch.nn.Module,\n","    ):\n","        self._orig_ds = ds\n","        self._get_el = get_el\n","        self._audio_tf = audio_transforms\n","        self._text_tf = text_transforms\n","\n","    def __getitem__(self, idx):\n","        audio, text = self._get_el(self._orig_ds, idx)\n","        audio = self._audio_tf(audio)\n","        text = self._text_tf(text)\n","        return audio, text\n","\n","    def __len__(self):\n","        return len(self._orig_ds)\n","\n","\n","def _get_ljspeech_el(\n","    ds: torch.utils.data.Dataset, idx: int\n",") -> Tuple[torch.Tensor, torch.Tensor]:\n","    el = ds[idx]\n","    return el[0], el[-1]\n","\n","\n","def _log_mel_stft_transforms(config: config_dict.ConfigDict) -> torch.nn.Module:\n","    def log_mel(x):\n","        log_x = torch.clamp(x, min=1e-10).log10()\n","        log_x = torch.maximum(log_x, log_x.max() - 8.0)\n","        log_x = (log_x + 4.0) / 4.0\n","        return log_x\n","\n","    return torchvision.transforms.Compose(\n","        [\n","            functools.partial(torch.squeeze, dim=0),\n","            torchaudio.transforms.Spectrogram(**config.spectrogram),\n","            torchaudio.transforms.MelScale(**config.mel_scale),\n","            log_mel,\n","            torchvision.transforms.Pad(\n","                (config.pad.l, config.pad.t, config.pad.r, config.pad.b)\n","            ),\n","            lambda x: x[..., :, : config.pad.r],\n","        ]\n","    )\n","\n","\n","def build_dl(\n","    split: DataSplit, config: config_dict.ConfigDict\n",") -> torch.utils.data.DataLoader:\n","    os.makedirs(config.data_folder, exist_ok=True)\n","    orig_ds = torchaudio.datasets.LJSPEECH(config.data_folder, download=True)\n","    get_el = _get_ljspeech_el\n","    audio_tf = _log_mel_stft_transforms(config.transforms.audio)\n","    text_tf = TokenVectorizer(**config.transforms.text.token_vectorizer)\n","\n","    ds = torch.utils.data.Subset(orig_ds, range(*config.data_split.get(split)))\n","    ds = SpeechRecognitionDataset(ds, get_el, audio_tf, text_tf)\n","    dl = torch.utils.data.DataLoader(ds, **config.dataloader.get(split))\n","    return dl\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-15T17:36:33.513731Z","iopub.status.busy":"2023-12-15T17:36:33.513328Z","iopub.status.idle":"2023-12-15T17:36:33.566090Z","shell.execute_reply":"2023-12-15T17:36:33.565173Z","shell.execute_reply.started":"2023-12-15T17:36:33.513705Z"},"id":"2VJPt6aLpoBA","trusted":true},"outputs":[],"source":["# hw2/model.py\n","\n","from typing import Any, Optional, Tuple\n","\n","from ml_collections import config_dict\n","import numpy as np\n","import torch\n","\n","\n","class SpeechToText(torch.nn.Module):\n","    def __init__(\n","        self,\n","        max_text_len: int,\n","        encoder_kwargs: config_dict.ConfigDict,\n","        decoder_type: DecoderType,\n","        decoder_kwargs: config_dict.ConfigDict,\n","    ) -> None:\n","        \"\"\"\n","        text_freq_dim is the vocabulary size.\n","        \"\"\"\n","        super().__init__()\n","        self._max_text_len = max_text_len\n","        self._enc = AudioEncoderTransformer(**encoder_kwargs)\n","        self._dec, self._single_input = self._decoder_factory(\n","            decoder_type, decoder_kwargs\n","        )\n","\n","    def forward(\n","        self, audio_feat: torch.Tensor, text_feat: torch.Tensor\n","    ) -> torch.Tensor:\n","        return self._dec(text_feat, self._enc(audio_feat))\n","\n","    @torch.no_grad()\n","    def generate(self, audio_feat: torch.Tensor) -> torch.Tensor:\n","        enc_feat = self._enc(audio_feat)\n","        text = last_token = torch.zeros(\n","            (enc_feat.shape[0], 1), dtype=torch.long, device=enc_feat.device\n","        )  # vocab index of start token `<` = 0\n","        start_token = True\n","\n","        for _ in range(self._max_text_len - 1):\n","            dec_input = last_token if self._single_input else text\n","            logits = self._dec.generate(dec_input, enc_feat, start_token)\n","            tokens = torch.argmax(logits, dim=-2)\n","            last_token = tokens[:, -1:]\n","            text = torch.concat([text, last_token], axis=-1)\n","            start_token = False\n","\n","        return text\n","\n","    def _decoder_factory(\n","        self, decoder_type: DecoderType, kwargs: config_dict.ConfigDict\n","    ) -> torch.nn.Module:\n","        if decoder_type == DecoderType.RNN:\n","            return TextDecoderRecurrent(**kwargs), True\n","        elif decoder_type == DecoderType.TRANSFORMER:\n","            return TextDecoderTransformer(**kwargs), False\n","        else:\n","            raise ValueError(f\"Decoder type `{decoder_type}` is not valid.\")\n","\n","\n","class AudioEncoderTransformer(torch.nn.Module):\n","    def __init__(\n","        self,\n","        n_layers: int = 2,\n","        freq_dim: int = 129,\n","        time_dim: int = 2754,\n","        hidden_dim: int = 64,\n","        n_heads: int = 2,\n","        feed_fwd_dim: int = 128,\n","        **kwargs,\n","    ) -> None:\n","        super().__init__()\n","        del kwargs\n","        self._embed = SpeechEmbedding(freq_dim, hidden_dim)  # audio embed\n","        self.register_buffer(\"_pos_embed\", self._pos_encoding(time_dim, hidden_dim))\n","        self._att_blocks = torch.nn.ModuleList(\n","            [\n","                ResidualAttentionBlock(hidden_dim, n_heads, feed_fwd_dim)\n","                for _ in range(n_layers)\n","            ]\n","        )\n","        self._out_norm = torch.nn.LayerNorm(hidden_dim)\n","\n","    def forward(self, x: torch.Tensor) -> torch.Tensor:\n","        out = x  # (B, F, T)\n","        out = self._embed(out)\n","        out = out + self._pos_embed\n","\n","        for block in self._att_blocks:\n","            out = block(out)\n","\n","        out = self._out_norm(out)\n","        return out  # (B, T, E)\n","\n","    def _pos_encoding(\n","        self, length: int, channels: int, max_timescale: int = 3000\n","    ) -> torch.Tensor:\n","        \"\"\"Returns sinusoids for positional embedding\"\"\"\n","        assert channels % 2 == 0\n","        log_timescale_increment = np.log(max_timescale) / (channels // 2 - 1)\n","        inv_timescales = torch.exp(\n","            -log_timescale_increment * torch.arange(channels // 2)\n","        )\n","        scaled_time = (\n","            torch.arange(length)[:, np.newaxis] * inv_timescales[np.newaxis, :]\n","        )\n","        return torch.cat([torch.sin(scaled_time), torch.cos(scaled_time)], dim=1)\n","\n","\n","class TextDecoderTransformer(torch.nn.Module):\n","    def __init__(\n","        self,\n","        freq_dim: int = 35,\n","        time_dim: int = 50,\n","        hidden_dim: int = 64,\n","        n_heads: int = 2,\n","        feed_fwd_dim: int = 128,\n","        **kwargs,\n","    ) -> None:\n","        super().__init__()\n","        del kwargs\n","        self._embed = torch.nn.Embedding(freq_dim, hidden_dim)\n","        self._pos_embed = torch.nn.Parameter(torch.rand(time_dim - 1, hidden_dim))\n","        self._att = ResidualAttentionLayer(hidden_dim, n_heads, feed_fwd_dim)\n","        self._cross_att = ResidualAttentionBlock(hidden_dim, n_heads, feed_fwd_dim)\n","        self._out_norm = torch.nn.LayerNorm(hidden_dim)\n","        self._classifier = torch.nn.Linear(hidden_dim, freq_dim)\n","        mask = torch.empty(time_dim, time_dim).fill_(-np.inf).triu_(1)\n","        self.register_buffer(\"_mask\", mask, persistent=False)\n","\n","    def forward(self, x: torch.Tensor, latent: torch.Tensor) -> torch.Tensor:\n","        # in: x=(B, T_d), T_d: text length (max: 50 tokens)\n","        # in: latent=(B, T_e, H_e), see AudioEncoderTransformer\n","        # out: (B, F_d, T_d), F_d: text freq = num of vocab tokens\n","        # NOTE: when adding `self._pos_embed` don't forget to index its first dimension to have the same size as the input's text length `T_d`.\n","        \n","        # Embedding Layer\n","        embedded_input = self._embed(x)\n","\n","        # Add Positional Embedding\n","        positional_embeddings = self._pos_embed[:x.size(1)]\n","        embedded_input = embedded_input + positional_embeddings\n","\n","        # Residual Attention Layer\n","        attention_out = self._att(embedded_input, mask=self._mask)\n","\n","        # Cross Attention Layer\n","        cross_att_out = self._cross_att(attention_out, latent)\n","\n","        # Apply Layer Norm\n","        norm_out = self._out_norm(cross_att_out)\n","\n","        # Classifier Layer\n","        out = self._classifier(norm_out)\n","        out = out.permute(0, 2, 1)\n","\n","        return out\n","\n","    def generate(\n","        self, x: torch.Tensor, latent: torch.Tensor, start_token: bool = False\n","    ) -> torch.Tensor:\n","        return self.forward(x, latent)\n","\n","\n","class TextDecoderRecurrent(torch.nn.Module):\n","    def __init__(\n","        self,\n","        n_layers: int = 2,\n","        freq_dim: int = 35,\n","        hidden_dim: int = 64,\n","        n_heads: int = 2,\n","        feed_fwd_dim: int = 128,\n","        **kwargs,\n","    ) -> None:\n","        super().__init__()\n","        del kwargs\n","        self._embed = torch.nn.Embedding(freq_dim, hidden_dim)\n","        self._rnn_norm = torch.nn.LayerNorm(hidden_dim)\n","        self._rnn = torch.nn.LSTM(\n","            hidden_dim, hidden_dim, num_layers=n_layers, batch_first=True\n","        )\n","        self._cross_att = ResidualAttentionBlock(hidden_dim, n_heads, feed_fwd_dim)\n","        self._out_norm = torch.nn.LayerNorm(hidden_dim)\n","        self._classifier = torch.nn.Linear(hidden_dim, freq_dim)\n","        self._state = None\n","        \n","    def forward(\n","        self, x: torch.Tensor, latent: torch.Tensor, state: Any = None\n","    ) -> torch.Tensor:\n","        return self._forward(x, latent, state)[0]\n","\n","    def generate(\n","        self, x: torch.Tensor, latent: torch.Tensor, start_token: bool = False\n","    ) -> torch.Tensor:\n","        if start_token:\n","            self._state = None\n","\n","        out, self._state = self._forward(x, latent, self._state)\n","        return out\n","\n","    def _forward(\n","        self, x: torch.Tensor, latent: torch.Tensor, state: Any\n","    ) -> Tuple[torch.Tensor, Any]:\n","        # in: x=(B, T_d), T_d: text length (50 tokens w/ padding if needed)\n","        # in: latent=(B, T_e, H_e), see AudioEncoderTransformer\n","        # in: state=((N, B, H_d), (N, B, H_d)), N: number of RNN layers (see __init__), H_d: hidden dim of RNN\n","        # out is a tuple\n","        # out[0]: (B, F_d, T_d), F_d: text freq = num of vocab tokens\n","        # out[1]: rnn's final state\n","\n","        # Embedding Layer\n","        embedded_input = self._embed(x)\n","\n","        # Apply Layer Norm\n","        # embedded_input = self._rnn_norm(embedded_input)\n","\n","        # RNN Layer\n","        rnn_out, rnn_state = self._rnn(embedded_input, state)\n","\n","        # Apply Layer Norm\n","        # rnn_out = self._rnn_norm(rnn_out)\n","\n","        # Cross Attention Layer\n","        att_output, *alignment = self._cross_att(latent, rnn_out)\n","\n","        # Apply Layer Norm\n","        norm_out = self._out_norm(rnn_out)\n","\n","        # Classifier Layer\n","        out = self._classifier(norm_out)\n","        out = out.permute(0, 2, 1)\n","\n","        return out, rnn_state\n","\n","\n","\n","\n","class ResidualAttentionBlock(torch.nn.Module):\n","    def __init__(\n","        self,\n","        hidden_dim: int = 64,\n","        n_heads: int = 2,\n","        feed_fwd_dim: int = 128\n","    ) -> None:\n","        super().__init__()\n","        self._att_norm = torch.nn.LayerNorm(hidden_dim)\n","        self._att = MultiHeadAttention(hidden_dim, n_heads)\n","        self._mlp_norm = torch.nn.LayerNorm(hidden_dim)\n","        self._mlp = torch.nn.Sequential(\n","            torch.nn.Linear(hidden_dim, feed_fwd_dim),\n","            torch.nn.GELU(),\n","            torch.nn.Linear(feed_fwd_dim, hidden_dim),\n","        )\n","\n","    def forward(\n","        self, x: torch.Tensor, cross_x: Optional[torch.Tensor] = None\n","    ) -> torch.Tensor:\n","        out = x\n","        out = out + self._att(self._att_norm(out), cross_x)\n","        out = out + self._mlp(self._mlp_norm(out))\n","        return out\n","    \n","\n","class ResidualAttentionLayer(torch.nn.Module):\n","    def __init__(\n","        self, hidden_dim: int = 64, n_heads: int = 2, feed_fwd_dim: int = 128\n","    ) -> None:\n","        super().__init__()\n","        self._att_norm = torch.nn.LayerNorm(hidden_dim)\n","        self._att = MultiHeadAttention(hidden_dim, n_heads)\n","\n","    def forward(\n","        self, x: torch.Tensor, mask: Optional[torch.Tensor] = None,\n","    ) -> torch.Tensor:\n","        out = x\n","        out = out + self._att(self._att_norm(out), mask=mask)\n","        return out\n","\n","\n","class MultiHeadAttention(torch.nn.Module):\n","    \"\"\"Simple multi head attn implementation, torch's version has unnecessary overhead.\"\"\"\n","\n","    def __init__(self, hidden_dim: int, n_heads: int):\n","        super().__init__()\n","        self._n_heads = n_heads\n","        self._query = torch.nn.Linear(hidden_dim, hidden_dim)\n","        self._key = torch.nn.Linear(hidden_dim, hidden_dim, bias=False)\n","        self._value = torch.nn.Linear(hidden_dim, hidden_dim)\n","        self._out = torch.nn.Linear(hidden_dim, hidden_dim)\n","\n","    def forward(\n","        self,\n","        x: torch.Tensor,\n","        xa: Optional[torch.Tensor] = None,\n","        mask: Optional[torch.Tensor] = None,\n","    ):\n","        q = self._query(x)\n","        k = self._key(x if xa is None else xa)\n","        v = self._value(x if xa is None else xa)\n","        wv = self._qkv_attention(q, k, v, mask)\n","        return self._out(wv)\n","\n","    def _qkv_attention(\n","        self,\n","        q: torch.Tensor,\n","        k: torch.Tensor,\n","        v: torch.Tensor,\n","        mask: Optional[torch.Tensor] = None,\n","    ):\n","        _, n_seq, hidden_dim = q.shape\n","        scale = (hidden_dim // self._n_heads) ** -0.25\n","        q = q.view(*q.shape[:2], self._n_heads, -1).permute(0, 2, 1, 3) * scale\n","        k = k.view(*k.shape[:2], self._n_heads, -1).permute(0, 2, 3, 1) * scale\n","        v = v.view(*v.shape[:2], self._n_heads, -1).permute(0, 2, 1, 3)\n","\n","        qk = q @ k\n","\n","        if mask is not None:\n","            qk = qk + mask[:n_seq, :n_seq]\n","\n","        qk = qk.float()\n","        w = torch.nn.functional.softmax(qk, dim=-1).to(q.dtype)\n","        return (w @ v).permute(0, 2, 1, 3).flatten(start_dim=2)\n","\n","\n","class SpeechEmbedding(torch.nn.Module):\n","    def __init__(\n","        self, in_dim: int = 129, hidden_dim: int = 64, kernel: int = 3\n","    ) -> None:\n","        super().__init__()\n","        self._embed = torch.nn.Sequential(\n","            torch.nn.Conv1d(in_dim, hidden_dim, kernel_size=kernel, padding=1),\n","            torch.nn.GELU(),\n","            torch.nn.Conv1d(\n","                hidden_dim, hidden_dim, kernel_size=kernel, stride=2, padding=1\n","            ),\n","            torch.nn.GELU(),\n","        )\n","\n","    def forward(self, x: torch.Tensor) -> torch.Tensor:\n","        \"\"\"\n","        x : torch.Tensor, shape = (bs, freq, time), freq can be n_fft (when converting\n","            audio into spectrogram) or n_mels (when converting audio into mel\n","            spectrogram).\n","        \"\"\"\n","        out = x\n","        out = self._embed(out)\n","        out = out.permute(0, 2, 1)\n","        return out\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-15T17:36:33.568087Z","iopub.status.busy":"2023-12-15T17:36:33.567715Z","iopub.status.idle":"2023-12-15T17:36:33.614167Z","shell.execute_reply":"2023-12-15T17:36:33.613361Z","shell.execute_reply.started":"2023-12-15T17:36:33.568055Z"},"id":"VL6vg3hwpqls","trusted":true},"outputs":[],"source":["# hw2/trainer.py\n","\n","import os\n","import pathlib as pl\n","from typing import Callable, Dict, List, Tuple, Union\n","\n","from ml_collections import config_dict\n","import numpy as np\n","import textdistance as td\n","import torch\n","\n","\n","class Trainer:\n","    def __init__(self, config: config_dict.ConfigDict, device: str = \"cpu\"):\n","        self._device = torch.device(device)\n","        self._config = config\n","        self._save_step = np.linspace(0, config.epochs, num=10, dtype=np.int32)\n","        self._model = SpeechToText(**config.speech_to_text)\n","        self._model.to(self._device)\n","        self._opt = torch.optim.AdamW(self._model.parameters(), lr=config.lr)\n","        warm_up_sch = torch.optim.lr_scheduler.LinearLR(\n","            self._opt, 1e-2, 1, int(0.15 * config.epochs)\n","        )\n","        decay_sch = torch.optim.lr_scheduler.LinearLR(\n","            self._opt, 1, 1e-2, int(0.85 * config.epochs)\n","        )\n","        self._opt_sch = torch.optim.lr_scheduler.SequentialLR(\n","            self._opt, [warm_up_sch, decay_sch], [int(0.15 * config.epochs)]\n","        )\n","        self._run_loss = 0.0\n","        self._log_train_step_int = 30\n","        self._prev_log_step = 0\n","\n","        self._text_vec = TokenVectorizer(\n","            **config.data.transforms.text.token_vectorizer\n","        )\n","        self._stop_token_idx = self._text_vec.mapper[self._text_vec._stop]\n","        self._empty_token = self._text_vec._empty\n","        self._str_algos = {\n","            \"jaccard_similarity\": td.jaccard.normalized_similarity,\n","            \"cosine_similarity\": td.cosine.normalized_similarity,\n","            \"damerau-levenshtein_similarity\": td.damerau_levenshtein.normalized_similarity,\n","        }\n","        self._metrics = []\n","        self._storage_path = pl.Path(self._config.storage_folder)\n","        self._storage_path /= str(self._config.seed)\n","        os.makedirs(self._storage_path, exist_ok=True)\n","\n","    def train(self) -> None:\n","        train_dl = build_dl(DataSplit.TRAIN, self._config.data)\n","        val_dl = build_dl(DataSplit.VALIDATION, self._config.data)\n","\n","        for epoch in range(self._config.epochs):\n","            metrics = {}\n","            self._prev_log_step = 0\n","            print(f\"[{epoch + 1}]\\ntrain:\")\n","\n","            for step, data in enumerate(train_dl):\n","                audio, text = data[0].to(self._device), data[1].to(self._device)\n","                train_metrics = self._train_step(audio, text)\n","                metrics = self._upt_metrics(metrics, train_metrics, \"train\")\n","                self._log_train(epoch, step, metrics[\"train\"])\n","\n","            print(\"\\nvalidation:\")\n","\n","            for step, data in enumerate(val_dl):\n","                audio, text = data[0].to(self._device), data[1].to(self._device)\n","                val_metrics, log = self._eval_step(audio, text, step + 1 == len(val_dl))\n","                metrics = self._upt_metrics(metrics, val_metrics, \"val\")\n","\n","            self._log_val(step, metrics[\"val\"], log)\n","            self._opt_sch.step()\n","            self._metrics.append(metrics)\n","\n","            if epoch in self._save_step:\n","                model_file = self._storage_path\n","                model_file /= f\"audio_to_text__{epoch}.pt\"\n","                torch.save(self._model.state_dict(), (model_file))\n","\n","        metrics_file = self._storage_path\n","        metrics_file /= f\"train_metrics__{self._config.epochs}.pkl\"\n","        save_obj(self._metrics, metrics_file)\n","\n","    def test(self) -> None:\n","        test_dl = build_dl(DataSplit.TEST, self._config.data)\n","        metrics = {}\n","        log = []\n","\n","        for data in test_dl:\n","            audio, text = data[0].to(self._device), data[1].to(self._device)\n","            batch_metrics, batch_log = self._eval_step(audio, text, True)\n","            metrics = self._upt_metrics(metrics, batch_metrics, \"test\")\n","            log += batch_log\n","\n","        metrics_file = self._storage_path\n","        metrics_file /= f\"test_metrics__{self._config.epochs}.pkl\"\n","        save_obj(metrics, metrics_file)\n","\n","        log_file = self._storage_path\n","        log_file /= f\"test_log__{self._config.epochs}.txt\"\n","        save_txt(\"\".join(log), log_file)\n","\n","    def _train_step(self, audio: torch.Tensor, text: torch.Tensor) -> float:\n","        self._opt.zero_grad()\n","        loss = self._fwd_step(audio, text)\n","        loss.backward()\n","        self._opt.step()\n","        return {\"loss\": loss.item()}\n","\n","    @torch.no_grad()\n","    def _eval_step(\n","        self, audio: torch.Tensor, text: torch.Tensor, log_text: bool = False\n","    ) -> Tuple[Dict[str, float], str]:\n","        loss = self._fwd_step(audio, text).item()\n","        gen_text = self._model.generate(audio)\n","        text, gen_text = text.cpu().numpy(), gen_text.cpu().numpy()\n","        metrics = self._str_sim_algos(self._str_algos, text, gen_text)\n","        metrics[\"loss\"] = loss\n","        metrics[\"n\"] = text.shape[0]\n","\n","        if not log_text:\n","            return metrics, \"\"\n","\n","        return metrics, self._cmp_target_w_gen(text, gen_text)\n","\n","    def _fwd_step(self, audio: torch.Tensor, text: torch.Tensor) -> torch.Tensor:\n","        target = text[:, 1:]\n","        logits = self._model(audio, text[:, :-1])\n","        loss = torch.nn.functional.cross_entropy(logits, target, label_smoothing=0.1)\n","        return loss\n","\n","    def _create_dataset(self, split: DataSplit):\n","        return\n","\n","    def _log_train(self, epoch: int, step: int, metrics: Dict[str, float]) -> None:\n","        if step % self._log_train_step_int != self._log_train_step_int - 1:\n","            return\n","\n","        print(f\"[{epoch + 1}, {step + 1:4d}]\", end=\" \")\n","        n = step - self._prev_log_step + 1\n","\n","        for m, v in metrics.items():\n","            print(f\"{m}: {sum(v[self._prev_log_step:]) / n:.3f}\")\n","            self._prev_log_step = step + 1\n","\n","    def _log_val(\n","        self, step: int, metrics: Dict[str, Union[float, str]], log: str\n","    ) -> None:\n","        n = sum(metrics[\"n\"])\n","\n","        for k, v in metrics.items():\n","            if k == \"n\":\n","                continue\n","\n","            print(k)\n","            v = sum(v)\n","            v = v / n if k != \"loss\" else v / len(metrics[\"n\"])  # loss is already avg.\n","            print(f\"{v:.3f}\", end=\"\\n\\n\")\n","\n","        print(\"compare-text-gen:\")\n","        print(log, end=\"\\n\\n\")\n","\n","    def _upt_metrics(\n","        self, base: Dict[str, float], upt: Dict[str, float], phase: str\n","    ) -> Dict[str, float]:\n","        base_phase = base.get(phase, {})\n","\n","        for k in upt:\n","            if k not in base_phase:\n","                base_phase[k] = []\n","\n","            base_phase[k].append(upt[k])\n","\n","        base[phase] = base_phase\n","        return base\n","\n","    def _str_sim_algos(\n","        self, str_algos: Dict[str, Callable], target: np.ndarray, gen: np.ndarray\n","    ) -> Dict[str, float]:\n","        metrics = {k: 0 for k in str_algos}\n","\n","        for t, g in zip(target, gen):\n","            t, g = self._preprocess_text(t, g)\n","\n","            for k, algo_fn in str_algos.items():\n","                metrics[k] += algo_fn(t, g)\n","\n","        return metrics\n","\n","    def _cmp_target_w_gen(self, target: np.ndarray, gen: np.ndarray) -> str:\n","        cmp = []\n","\n","        for t, g in zip(target, gen):\n","            t = \"\".join([self._text_vec.invert_mapper[c] for c in t])\n","            t = t.replace(\"@\", \"\")\n","            last_idx = np.where(g == 1)[0]\n","            last_idx = last_idx[0] + 1 if last_idx.size > 0 else g.shape[0]\n","            g = g[:last_idx]\n","            g = \"\".join([self._text_vec.invert_mapper[c] for c in g])\n","            cmp += [\"target: \", t, \"\\n\", \"pred:   \", g, \"\\n\\n\"]\n","\n","        return \"\".join(cmp)\n","\n","    def _preprocess_text(\n","        self, target: np.ndarray, gen: np.ndarray\n","    ) -> Tuple[List[str], List[str]]:\n","        last_idx = np.where((target == self._stop_token_idx))[0][0]\n","        target = target[: last_idx + 1]\n","        last_idx = np.where(gen == 1)[0]\n","        last_idx = last_idx[0] if last_idx.size > 0 else len(gen) - 1\n","        gen[last_idx] = 1\n","        gen = gen[: last_idx + 1]\n","        return target.tolist(), gen.tolist()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-15T17:36:33.615843Z","iopub.status.busy":"2023-12-15T17:36:33.615467Z","iopub.status.idle":"2023-12-15T17:36:33.624513Z","shell.execute_reply":"2023-12-15T17:36:33.623592Z","shell.execute_reply.started":"2023-12-15T17:36:33.615809Z"},"trusted":true},"outputs":[],"source":["# main.py\n","\n","import random\n","\n","\n","def plotting(config):\n","    epochs, seed = config.epochs, config.seed\n","    metrics_path = pl.Path(config.storage_folder) / str(seed)\n","    plot_path = metrics_path / \"plots\"\n","    os.makedirs(plot_path, exist_ok=True)\n","\n","    labels = {\"train\": \"train\", \"val\": \"validation\"}\n","    train_metrics_path = metrics_path / f\"train_metrics__{epochs}.pkl\"\n","    train_metrics = load_metrics(train_metrics_path)\n","    plot_metrics(train_metrics, plot_path, \"train\")\n","    plot_metrics(train_metrics, plot_path, \"val\")\n","    cmp_phase_metrics(train_metrics, \"loss\", [\"train\", \"val\"], plot_path, labels)\n","\n","    test_metrics_path = metrics_path / f\"test_metrics__{epochs}.pkl\"\n","    test_metrics = load_metrics(test_metrics_path)[\"test\"]\n","    mean_test_metrics = {}\n","\n","    for k, v in test_metrics.items():\n","        if k == \"n\":\n","            continue\n","\n","        if k == \"loss\":\n","            mean_test_metrics[k] = sum(v) / len(test_metrics[\"n\"])\n","        else:\n","            mean_test_metrics[k] = sum(v) / sum(test_metrics[\"n\"])\n","\n","    print(\"test:\")\n","    print(mean_test_metrics)"]},{"cell_type":"markdown","metadata":{},"source":["## (a)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-15T17:36:44.651096Z","iopub.status.busy":"2023-12-15T17:36:44.650733Z","iopub.status.idle":"2023-12-15T17:59:45.904767Z","shell.execute_reply":"2023-12-15T17:59:45.903736Z","shell.execute_reply.started":"2023-12-15T17:36:44.651067Z"},"trusted":true},"outputs":[],"source":["if __name__ == \"__main__\":\n","    # not using absl (no need for command line), get config manually\n","    config = get_config()\n","    config.unlock()  # update hyperparams before `config.lock()`\n","    config.storage_folder = \"./storage/attention-rnn\"\n","    config.speech_to_text.decoder_type = DecoderType.RNN\n","    config.speech_to_text.decoder_kwargs = dict(\n","        n_layers=1,\n","        freq_dim=config.get_ref(\"n_vocab\"),\n","        time_dim=config.get_ref(\"text_len\"),\n","        hidden_dim=200,\n","        n_heads=2,\n","        feed_fwd_dim=400\n","    )\n","    config.lock()\n","\n","    # Manual seed\n","    torch.manual_seed(config.seed)\n","    np.random.seed(config.seed)\n","    random.seed(config.seed)\n","\n","    # Device to use\n","    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","    print(f\"Using {DEVICE}\")\n","\n","    # Training & testing\n","    trainer = Trainer(config, DEVICE)\n","    trainer.train()\n","    trainer.test()\n","\n","    # Plotting - check './storage/attention-rnn/{seed}/plots\n","    plotting(config)"]},{"cell_type":"markdown","metadata":{},"source":["## (b)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-15T18:01:22.385134Z","iopub.status.busy":"2023-12-15T18:01:22.384568Z","iopub.status.idle":"2023-12-15T18:16:50.079656Z","shell.execute_reply":"2023-12-15T18:16:50.078607Z","shell.execute_reply.started":"2023-12-15T18:01:22.385099Z"},"id":"nwz_4GeR1GLN","outputId":"e68a6bac-3962-46f2-fb51-093692c8377d","trusted":true},"outputs":[],"source":["if __name__ == \"__main__\":\n","    # not using absl (no need for command line), get config manually\n","    config = get_config()\n","    config.unlock() # update hyperparams before `config.lock()`\n","    config.storage_folder = \"./storage/attention-attention\"\n","    config.speech_to_text.decoder_type = DecoderType.TRANSFORMER\n","    config.speech_to_text.decoder_kwargs = dict(\n","        freq_dim=config.get_ref(\"n_vocab\"),\n","        time_dim=config.get_ref(\"text_len\"),\n","        hidden_dim=200,\n","        n_heads=2,\n","        feed_fwd_dim=400\n","    )\n","    config.lock()\n","\n","    # Manual seed\n","    torch.manual_seed(config.seed)\n","    np.random.seed(config.seed)\n","    random.seed(config.seed)\n","\n","    # Device to use\n","    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","    print(f\"Using {DEVICE}\")\n","\n","    # Training & testing\n","    trainer = Trainer(config, DEVICE)\n","    trainer.train()\n","    trainer.test()\n","\n","    # Plotting - check './storage/attention-attention/{seed}/plots\n","    plotting(config)"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30627,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":4}
